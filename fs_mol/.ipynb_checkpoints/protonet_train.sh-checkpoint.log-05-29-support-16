2024-05-29 22:08:30: Starting train run FSMol_ProtoNet_gnn+ecfp+fc_2024-05-29_22-08-30.
2024-05-29 22:08:30: 	Arguments: Namespace(DATA_PATH='/home/gridsan/ppaschalidis/Ersilia-FS/dataset/min_size_16', task_list_file='/home/gridsan/ppaschalidis/Ersilia-FS/dataset/min_size_16/entire_train_set.json', save_dir='outputs', seed=0, azureml_logging=False, features='gnn+ecfp+fc', distance_metric='mahalanobis', gnn_type='PNA', node_embed_dim=128, num_heads=4, per_head_dim=64, intermediate_dim=1024, message_function_depth=1, num_gnn_layers=10, readout_type='combined', readout_use_all_states=True, readout_num_heads=12, readout_head_dim=64, readout_output_dim=512, support_set_size=16, query_set_size=256, tasks_per_batch=16, batch_size=256, num_train_steps=10000, validate_every=50, validation_support_set_sizes=[16], validation_query_set_size=512, validation_num_samples=5, lr=0.0001, clip_value=None, pretrained_gnn=None, pretrained_pn=None)
2024-05-29 22:08:30: 	Output dir: outputs/FSMol_ProtoNet_gnn+ecfp+fc_2024-05-29_22-08-30
2024-05-29 22:08:30: 	Data path: /home/gridsan/ppaschalidis/Ersilia-FS/dataset/min_size_16
2024-05-29 22:08:30: Identified 655 training tasks.
2024-05-29 22:08:30: Identified 34 validation tasks.
2024-05-29 22:08:30: Identified 200 test tasks.
2024-05-29 22:08:30: 	Device: cpu
2024-05-29 22:08:30: 	Num parameters 19042338
2024-05-29 22:08:30: 	Model:
PrototypicalNetworkTrainer(
  (graph_feature_extractor): GraphFeatureExtractor(
    (init_node_proj): Linear(in_features=32, out_features=128, bias=False)
    (gnn): GNN(
      (gnn_blocks): ModuleList(
        (0): GNNBlock(
          (mp_layers): ModuleList(
            (0): RelationalMultiAggrMP(
              (message_fns): ModuleList(
                (0): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
                (1): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
                (2): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
              )
            )
            (1): RelationalMultiAggrMP(
              (message_fns): ModuleList(
                (0): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
                (1): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
                (2): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
              )
            )
            (2): RelationalMultiAggrMP(
              (message_fns): ModuleList(
                (0): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
                (1): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
                (2): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
              )
            )
            (3): RelationalMultiAggrMP(
              (message_fns): ModuleList(
                (0): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
                (1): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
                (2): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
              )
            )
          )
          (msg_out_projection): Linear(in_features=3072, out_features=128, bias=True)
          (mp_norm_layer): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (boom_layer): BOOMLayer(
            (linear1): Linear(in_features=128, out_features=1024, bias=True)
            (activation): LeakyReLU(negative_slope=0.01)
            (dropout): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=128, bias=True)
          )
          (boom_norm_layer): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (dropout_layer): Dropout(p=0.0, inplace=False)
        )
        (1): GNNBlock(
          (mp_layers): ModuleList(
            (0): RelationalMultiAggrMP(
              (message_fns): ModuleList(
                (0): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
                (1): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
                (2): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
              )
            )
            (1): RelationalMultiAggrMP(
              (message_fns): ModuleList(
                (0): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
                (1): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
                (2): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
              )
            )
            (2): RelationalMultiAggrMP(
              (message_fns): ModuleList(
                (0): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
                (1): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
                (2): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
              )
            )
            (3): RelationalMultiAggrMP(
              (message_fns): ModuleList(
                (0): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
                (1): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
                (2): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
              )
            )
          )
          (msg_out_projection): Linear(in_features=3072, out_features=128, bias=True)
          (mp_norm_layer): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (boom_layer): BOOMLayer(
            (linear1): Linear(in_features=128, out_features=1024, bias=True)
            (activation): LeakyReLU(negative_slope=0.01)
            (dropout): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=128, bias=True)
          )
          (boom_norm_layer): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (dropout_layer): Dropout(p=0.0, inplace=False)
        )
        (2): GNNBlock(
          (mp_layers): ModuleList(
            (0): RelationalMultiAggrMP(
              (message_fns): ModuleList(
                (0): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
                (1): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
                (2): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
              )
            )
            (1): RelationalMultiAggrMP(
              (message_fns): ModuleList(
                (0): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
                (1): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
                (2): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
              )
            )
            (2): RelationalMultiAggrMP(
              (message_fns): ModuleList(
                (0): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
                (1): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
                (2): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
              )
            )
            (3): RelationalMultiAggrMP(
              (message_fns): ModuleList(
                (0): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
                (1): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
                (2): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
              )
            )
          )
          (msg_out_projection): Linear(in_features=3072, out_features=128, bias=True)
          (mp_norm_layer): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (boom_layer): BOOMLayer(
            (linear1): Linear(in_features=128, out_features=1024, bias=True)
            (activation): LeakyReLU(negative_slope=0.01)
            (dropout): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=128, bias=True)
          )
          (boom_norm_layer): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (dropout_layer): Dropout(p=0.0, inplace=False)
        )
        (3): GNNBlock(
          (mp_layers): ModuleList(
            (0): RelationalMultiAggrMP(
              (message_fns): ModuleList(
                (0): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
                (1): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
                (2): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
              )
            )
            (1): RelationalMultiAggrMP(
              (message_fns): ModuleList(
                (0): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
                (1): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
                (2): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
              )
            )
            (2): RelationalMultiAggrMP(
              (message_fns): ModuleList(
                (0): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
                (1): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
                (2): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
              )
            )
            (3): RelationalMultiAggrMP(
              (message_fns): ModuleList(
                (0): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
                (1): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
                (2): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
              )
            )
          )
          (msg_out_projection): Linear(in_features=3072, out_features=128, bias=True)
          (mp_norm_layer): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (boom_layer): BOOMLayer(
            (linear1): Linear(in_features=128, out_features=1024, bias=True)
            (activation): LeakyReLU(negative_slope=0.01)
            (dropout): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=128, bias=True)
          )
          (boom_norm_layer): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (dropout_layer): Dropout(p=0.0, inplace=False)
        )
        (4): GNNBlock(
          (mp_layers): ModuleList(
            (0): RelationalMultiAggrMP(
              (message_fns): ModuleList(
                (0): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
                (1): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
                (2): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
              )
            )
            (1): RelationalMultiAggrMP(
              (message_fns): ModuleList(
                (0): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
                (1): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
                (2): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
              )
            )
            (2): RelationalMultiAggrMP(
              (message_fns): ModuleList(
                (0): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
                (1): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
                (2): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
              )
            )
            (3): RelationalMultiAggrMP(
              (message_fns): ModuleList(
                (0): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
                (1): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
                (2): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
              )
            )
          )
          (msg_out_projection): Linear(in_features=3072, out_features=128, bias=True)
          (mp_norm_layer): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (boom_layer): BOOMLayer(
            (linear1): Linear(in_features=128, out_features=1024, bias=True)
            (activation): LeakyReLU(negative_slope=0.01)
            (dropout): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=128, bias=True)
          )
          (boom_norm_layer): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (dropout_layer): Dropout(p=0.0, inplace=False)
        )
        (5): GNNBlock(
          (mp_layers): ModuleList(
            (0): RelationalMultiAggrMP(
              (message_fns): ModuleList(
                (0): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
                (1): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
                (2): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
              )
            )
            (1): RelationalMultiAggrMP(
              (message_fns): ModuleList(
                (0): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
                (1): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
                (2): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
              )
            )
            (2): RelationalMultiAggrMP(
              (message_fns): ModuleList(
                (0): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
                (1): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
                (2): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
              )
            )
            (3): RelationalMultiAggrMP(
              (message_fns): ModuleList(
                (0): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
                (1): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
                (2): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
              )
            )
          )
          (msg_out_projection): Linear(in_features=3072, out_features=128, bias=True)
          (mp_norm_layer): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (boom_layer): BOOMLayer(
            (linear1): Linear(in_features=128, out_features=1024, bias=True)
            (activation): LeakyReLU(negative_slope=0.01)
            (dropout): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=128, bias=True)
          )
          (boom_norm_layer): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (dropout_layer): Dropout(p=0.0, inplace=False)
        )
        (6): GNNBlock(
          (mp_layers): ModuleList(
            (0): RelationalMultiAggrMP(
              (message_fns): ModuleList(
                (0): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
                (1): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
                (2): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
              )
            )
            (1): RelationalMultiAggrMP(
              (message_fns): ModuleList(
                (0): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
                (1): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
                (2): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
              )
            )
            (2): RelationalMultiAggrMP(
              (message_fns): ModuleList(
                (0): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
                (1): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
                (2): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
              )
            )
            (3): RelationalMultiAggrMP(
              (message_fns): ModuleList(
                (0): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
                (1): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
                (2): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
              )
            )
          )
          (msg_out_projection): Linear(in_features=3072, out_features=128, bias=True)
          (mp_norm_layer): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (boom_layer): BOOMLayer(
            (linear1): Linear(in_features=128, out_features=1024, bias=True)
            (activation): LeakyReLU(negative_slope=0.01)
            (dropout): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=128, bias=True)
          )
          (boom_norm_layer): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (dropout_layer): Dropout(p=0.0, inplace=False)
        )
        (7): GNNBlock(
          (mp_layers): ModuleList(
            (0): RelationalMultiAggrMP(
              (message_fns): ModuleList(
                (0): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
                (1): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
                (2): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
              )
            )
            (1): RelationalMultiAggrMP(
              (message_fns): ModuleList(
                (0): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
                (1): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
                (2): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
              )
            )
            (2): RelationalMultiAggrMP(
              (message_fns): ModuleList(
                (0): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
                (1): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
                (2): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
              )
            )
            (3): RelationalMultiAggrMP(
              (message_fns): ModuleList(
                (0): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
                (1): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
                (2): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
              )
            )
          )
          (msg_out_projection): Linear(in_features=3072, out_features=128, bias=True)
          (mp_norm_layer): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (boom_layer): BOOMLayer(
            (linear1): Linear(in_features=128, out_features=1024, bias=True)
            (activation): LeakyReLU(negative_slope=0.01)
            (dropout): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=128, bias=True)
          )
          (boom_norm_layer): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (dropout_layer): Dropout(p=0.0, inplace=False)
        )
        (8): GNNBlock(
          (mp_layers): ModuleList(
            (0): RelationalMultiAggrMP(
              (message_fns): ModuleList(
                (0): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
                (1): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
                (2): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
              )
            )
            (1): RelationalMultiAggrMP(
              (message_fns): ModuleList(
                (0): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
                (1): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
                (2): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
              )
            )
            (2): RelationalMultiAggrMP(
              (message_fns): ModuleList(
                (0): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
                (1): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
                (2): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
              )
            )
            (3): RelationalMultiAggrMP(
              (message_fns): ModuleList(
                (0): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
                (1): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
                (2): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
              )
            )
          )
          (msg_out_projection): Linear(in_features=3072, out_features=128, bias=True)
          (mp_norm_layer): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (boom_layer): BOOMLayer(
            (linear1): Linear(in_features=128, out_features=1024, bias=True)
            (activation): LeakyReLU(negative_slope=0.01)
            (dropout): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=128, bias=True)
          )
          (boom_norm_layer): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (dropout_layer): Dropout(p=0.0, inplace=False)
        )
        (9): GNNBlock(
          (mp_layers): ModuleList(
            (0): RelationalMultiAggrMP(
              (message_fns): ModuleList(
                (0): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
                (1): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
                (2): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
              )
            )
            (1): RelationalMultiAggrMP(
              (message_fns): ModuleList(
                (0): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
                (1): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
                (2): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
              )
            )
            (2): RelationalMultiAggrMP(
              (message_fns): ModuleList(
                (0): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
                (1): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
                (2): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
              )
            )
            (3): RelationalMultiAggrMP(
              (message_fns): ModuleList(
                (0): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
                (1): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
                (2): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
              )
            )
          )
          (msg_out_projection): Linear(in_features=3072, out_features=128, bias=True)
          (mp_norm_layer): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (boom_layer): BOOMLayer(
            (linear1): Linear(in_features=128, out_features=1024, bias=True)
            (activation): LeakyReLU(negative_slope=0.01)
            (dropout): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=128, bias=True)
          )
          (boom_norm_layer): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (dropout_layer): Dropout(p=0.0, inplace=False)
        )
      )
    )
    (readout): CombinedGraphReadout(
      (_weighted_mean_pooler): MultiHeadWeightedGraphReadout(
        (_scoring_module): MLP(
          (_layers): Sequential(
            (0): Linear(in_features=1408, out_features=768, bias=True)
            (1): ReLU()
            (2): Linear(in_features=768, out_features=12, bias=True)
          )
        )
        (_transformation_mlp): MLP(
          (_layers): Sequential(
            (0): Linear(in_features=1408, out_features=768, bias=True)
            (1): ReLU()
            (2): Linear(in_features=768, out_features=768, bias=True)
          )
        )
        (_combination_layer): Linear(in_features=768, out_features=512, bias=False)
      )
      (_weighted_sum_pooler): MultiHeadWeightedGraphReadout(
        (_scoring_module): MLP(
          (_layers): Sequential(
            (0): Linear(in_features=1408, out_features=768, bias=True)
            (1): ReLU()
            (2): Linear(in_features=768, out_features=12, bias=True)
          )
        )
        (_transformation_mlp): MLP(
          (_layers): Sequential(
            (0): Linear(in_features=1408, out_features=768, bias=True)
            (1): ReLU()
            (2): Linear(in_features=768, out_features=768, bias=True)
          )
        )
        (_combination_layer): Linear(in_features=768, out_features=512, bias=False)
      )
      (_max_pooler): UnweightedGraphReadout(
        (_combination_layer): Linear(in_features=1408, out_features=512, bias=False)
      )
      (_combination_layer): Linear(in_features=1536, out_features=512, bias=False)
    )
  )
  (fc): Sequential(
    (0): Linear(in_features=2560, out_features=1024, bias=True)
    (1): ReLU()
    (2): Linear(in_features=1024, out_features=512, bias=True)
  )
)
2024-05-29 22:08:30: CHEMBL3285745:
2024-05-29 22:08:30: CHEMBL761987:
2024-05-29 22:08:30: CHEMBL3227331:
2024-05-29 22:08:30: CHEMBL703466:
2024-05-29 22:08:30: CHEMBL3761618:
2024-05-29 22:08:30: CHEMBL1047854:
2024-05-29 22:08:30: CHEMBL1794398:
2024-05-29 22:08:30: CHEMBL995355:
2024-05-29 22:08:30: CHEMBL3383460:
2024-05-29 22:08:30: CHEMBL833990:
2024-05-29 22:08:30: CHEMBL695986:
2024-05-29 22:08:30: CHEMBL2150628:
2024-05-29 22:08:30: CHEMBL863382:
2024-05-29 22:08:30: CHEMBL4002825:
2024-05-29 22:08:30: CHEMBL1020690:
2024-05-29 22:08:30: CHEMBL2162371:
2024-05-29 22:08:30: CHEMBL1177528:
2024-05-29 22:08:30: CHEMBL1259550:
2024-05-29 22:08:30: CHEMBL4197926:
2024-05-29 22:08:30: CHEMBL895182:
2024-05-29 22:08:30: CHEMBL3588714:
2024-05-29 22:08:30: CHEMBL3625422:
2024-05-29 22:08:30: CHEMBL1107214:
2024-05-29 22:08:30: CHEMBL804251:
2024-05-29 22:08:30: CHEMBL1068293:
2024-05-29 22:08:30: CHEMBL4187321:
2024-05-29 22:08:30: CHEMBL909968:
2024-05-29 22:08:30: CHEMBL3295662:
2024-05-29 22:08:30: CHEMBL1053167:
2024-05-29 22:08:30: CHEMBL3384578:
2024-05-29 22:08:30: CHEMBL1055019:
2024-05-29 22:08:30: CHEMBL1941610:
2024-05-29 22:08:30: CHEMBL3389542:
2024-05-29 22:08:30: CHEMBL3405593:
2024-05-29 22:08:30: CHEMBL1116600:
2024-05-29 22:08:30: CHEMBL2328533:
2024-05-29 22:08:30: CHEMBL1767670:
2024-05-29 22:08:30: CHEMBL4263092:
2024-05-29 22:08:30: CHEMBL4054609:
2024-05-29 22:08:30: CHEMBL956777:
2024-05-29 22:08:30: CHEMBL4432407:
2024-05-29 22:08:30: CHEMBL1034018:
2024-05-29 22:08:30: CHEMBL2447603:
2024-05-29 22:08:30: CHEMBL871033:
2024-05-29 22:08:30: CHEMBL1803360:
2024-05-29 22:08:30: CHEMBL2388705:
2024-05-29 22:08:30: CHEMBL941164:
2024-05-29 22:08:30: CHEMBL3266569:
2024-05-29 22:08:30: CHEMBL1107997:
2024-05-29 22:08:30: CHEMBL656397:
2024-05-29 22:08:30: CHEMBL1121169:
2024-05-29 22:08:30: CHEMBL922474:
2024-05-29 22:08:30: CHEMBL1960697:
2024-05-29 22:08:30: CHEMBL2051603:
2024-05-29 22:08:30: CHEMBL860970:
2024-05-29 22:08:30: CHEMBL3396060:
2024-05-29 22:08:30: CHEMBL2405783:
2024-05-29 22:08:30: CHEMBL710534:
2024-05-29 22:08:30: CHEMBL833931:
2024-05-29 22:08:30: CHEMBL2024927:
2024-05-29 22:08:30: CHEMBL1051810:
2024-05-29 22:08:30: CHEMBL1816524:
2024-05-29 22:08:30: CHEMBL937510:
2024-05-29 22:08:30: CHEMBL2051598:
2024-05-29 22:08:30: CHEMBL4136409:
2024-05-29 22:08:30: CHEMBL713255:
2024-05-29 22:08:30: CHEMBL4194391:
2024-05-29 22:08:30: CHEMBL1763053:
2024-05-29 22:08:30: CHEMBL3367829:
2024-05-29 22:08:30: CHEMBL930744:
2024-05-29 22:08:30: CHEMBL826375:
2024-05-29 22:08:30: CHEMBL2067378:
2024-05-29 22:08:30: CHEMBL889770:
2024-05-29 22:08:30: CHEMBL2033222:
2024-05-29 22:08:30: CHEMBL903139:
2024-05-29 22:08:30: CHEMBL3240111:
2024-05-29 22:08:31: CHEMBL1018957:
2024-05-29 22:08:31: CHEMBL712878:
2024-05-29 22:08:31: CHEMBL713409:
2024-05-29 22:08:31: CHEMBL708445:
2024-05-29 22:08:31: CHEMBL3295663:
2024-05-29 22:08:31: CHEMBL1260089:
2024-05-29 22:08:31: CHEMBL2072118:
2024-05-29 22:08:31: CHEMBL1228542:
2024-05-29 22:08:31: CHEMBL2148600:
2024-05-29 22:08:31: CHEMBL1654782:
2024-05-29 22:08:31: CHEMBL1815005:
2024-05-29 22:08:31: CHEMBL1654780:
2024-05-29 22:08:31: CHEMBL4719106:
2024-05-29 22:08:31: CHEMBL2341227:
2024-05-29 22:08:31: CHEMBL2019245:
2024-05-29 22:08:31: CHEMBL3067105:
2024-05-29 22:08:31: CHEMBL899618:
2024-05-29 22:08:31: CHEMBL2154555:
2024-05-29 22:08:31: CHEMBL1219015:
2024-05-29 22:08:31: CHEMBL902229:
2024-05-29 22:08:31: CHEMBL3578893:
2024-05-29 22:08:31: CHEMBL833275:
2024-05-29 22:08:31: CHEMBL3418331:
2024-05-29 22:08:31: CHEMBL3413997:
2024-05-29 22:08:31: CHEMBL1053306:
2024-05-29 22:08:31: CHEMBL833792:
2024-05-29 22:08:31: CHEMBL2445761:
2024-05-29 22:08:31: CHEMBL961554:
2024-05-29 22:08:31: CHEMBL1017067:
2024-05-29 22:08:31: CHEMBL2405555:
2024-05-29 22:08:31: CHEMBL3390310:
2024-05-29 22:08:31: CHEMBL1033996:
2024-05-29 22:08:31: CHEMBL1932094:
2024-05-29 22:08:31: CHEMBL889768:
2024-05-29 22:08:31: CHEMBL1052565:
2024-05-29 22:08:31: CHEMBL984423:
2024-05-29 22:08:31: CHEMBL689145:
2024-05-29 22:08:31: CHEMBL2379584:
2024-05-29 22:08:31: CHEMBL2210037:
2024-05-29 22:08:31: CHEMBL1817498:
2024-05-29 22:08:31: CHEMBL1930242:
2024-05-29 22:08:31: CHEMBL4253251:
2024-05-29 22:08:31: CHEMBL833117:
2024-05-29 22:08:31: CHEMBL1837558:
2024-05-29 22:08:31: CHEMBL1039164:
2024-05-29 22:08:31: CHEMBL3073615:
2024-05-29 22:08:31: CHEMBL995360:
2024-05-29 22:08:31: CHEMBL908714:
2024-05-29 22:08:31: CHEMBL1953907:
2024-05-29 22:08:31: CHEMBL906490:
2024-05-29 22:08:31: CHEMBL1056891:
2024-05-29 22:08:31: CHEMBL899467:
2024-05-29 22:08:31: CHEMBL833080:
2024-05-29 22:08:31: CHEMBL1794318:
2024-05-29 22:08:31: CHEMBL936680:
2024-05-29 22:08:31: CHEMBL1024879:
2024-05-29 22:08:31: CHEMBL3122259:
2024-05-29 22:08:32: CHEMBL2342597:
2024-05-29 22:08:32: CHEMBL3282588:
2024-05-29 22:08:32: CHEMBL3078730:
2024-05-29 22:08:33: CHEMBL860972:
2024-05-29 22:08:34: CHEMBL3295321:
2024-05-29 22:08:35: CHEMBL3815782:
2024-05-29 22:08:36: CHEMBL3088595:
2024-05-29 22:08:37: CHEMBL655136:
2024-05-29 22:08:38: CHEMBL3418917:
2024-05-29 22:08:39: CHEMBL924692:
2024-05-29 22:08:40: CHEMBL713254:
2024-05-29 22:08:41: CHEMBL1024326:
2024-05-29 22:08:41: CHEMBL1059735:
2024-05-29 22:08:42: CHEMBL683837:
2024-05-29 22:08:43: CHEMBL1816523:
2024-05-29 22:08:44: CHEMBL3129213:
2024-05-29 22:08:46: CHEMBL3077285:
2024-05-29 22:08:46: CHEMBL4477287:
2024-05-29 22:08:47: CHEMBL2390255:
2024-05-29 22:08:48: CHEMBL711656:
2024-05-29 22:08:48: CHEMBL3295320:
2024-05-29 22:08:49: CHEMBL758925:
2024-05-29 22:08:51: CHEMBL1035506:
2024-05-29 22:08:52: CHEMBL1054762:
2024-05-29 22:08:53: CHEMBL906554:
2024-05-29 22:08:54: CHEMBL1921081:
2024-05-29 22:08:55: CHEMBL3072372:
2024-05-29 22:08:56: CHEMBL698789:
2024-05-29 22:08:56: CHEMBL1109063:
2024-05-29 22:08:59: CHEMBL1073156:
2024-05-29 22:08:59: CHEMBL972376:
2024-05-29 22:09:01: CHEMBL2432161:
2024-05-29 22:09:02: CHEMBL1035508:
2024-05-29 22:09:04: CHEMBL2167912:
2024-05-29 22:09:05: CHEMBL3418701:
2024-05-29 22:09:06: CHEMBL1034017:
2024-05-29 22:09:07: CHEMBL2317095:
2024-05-29 22:09:08: CHEMBL1067060:
2024-05-29 22:09:09: CHEMBL2156781:
2024-05-29 22:09:10: CHEMBL1037279:
2024-05-29 22:09:10: CHEMBL711804:
2024-05-29 22:09:10: CHEMBL4006616:
2024-05-29 22:09:11: CHEMBL944781:
2024-05-29 22:09:12: CHEMBL833378:
2024-05-29 22:09:13: CHEMBL1663615:
2024-05-29 22:09:14: CHEMBL3295870:
2024-05-29 22:09:17: CHEMBL2183320:
2024-05-29 22:09:18: CHEMBL3122349:
2024-05-29 22:09:19: CHEMBL873604:
2024-05-29 22:09:20: CHEMBL1817497:
2024-05-29 22:09:22: CHEMBL944749:
2024-05-29 22:09:23: CHEMBL4220813:
2024-05-29 22:09:27: CHEMBL4002824:
2024-05-29 22:09:27: CHEMBL3592342:
2024-05-29 22:09:29: CHEMBL3129215:
2024-05-29 22:09:30: CHEMBL1918205:
2024-05-29 22:09:31: CHEMBL1006568:
2024-05-29 22:09:32: CHEMBL1918204:
2024-05-29 22:09:32: CHEMBL3123043:
2024-05-29 22:09:33: CHEMBL4179593:
2024-05-29 22:09:34: CHEMBL1045728:
2024-05-29 22:09:35: CHEMBL837535:
2024-05-29 22:09:36: CHEMBL4184680:
2024-05-29 22:09:37: CHEMBL955134:
2024-05-29 22:09:40: CHEMBL1769611:
2024-05-29 22:09:41: CHEMBL4050758:
2024-05-29 22:09:44: CHEMBL3994798:
2024-05-29 22:09:44: CHEMBL706945:
2024-05-29 22:09:45: CHEMBL2390645:
2024-05-29 22:09:46: CHEMBL907807:
2024-05-29 22:09:47: CHEMBL956847:
2024-05-29 22:09:48: CHEMBL1043900:
2024-05-29 22:09:49: CHEMBL946064:
2024-05-29 22:09:49: CHEMBL4264727:
2024-05-29 22:09:51: CHEMBL933048:
2024-05-29 22:09:51: CHEMBL1936129:
2024-05-29 22:09:55: CHEMBL1008279:
2024-05-29 22:09:56: CHEMBL863014:
2024-05-29 22:09:58: CHEMBL713546:
2024-05-29 22:09:59: CHEMBL922937:
2024-05-29 22:10:00: CHEMBL4276506:
2024-05-29 22:10:03: CHEMBL897415:
2024-05-29 22:10:04: CHEMBL968900:
2024-05-29 22:10:05: CHEMBL1832560:
2024-05-29 22:10:07: CHEMBL697276:
2024-05-29 22:10:08: CHEMBL833076:
2024-05-29 22:10:09: CHEMBL691343:
2024-05-29 22:10:10: CHEMBL1047137:
2024-05-29 22:10:12: CHEMBL984427:
2024-05-29 22:10:13: CHEMBL3096264:
2024-05-29 22:10:13: CHEMBL3122629:
2024-05-29 22:10:15: CHEMBL4019375:
2024-05-29 22:10:16: CHEMBL1664688:
2024-05-29 22:10:16: CHEMBL833855:
2024-05-29 22:10:16: CHEMBL759088:
2024-05-29 22:10:17: CHEMBL1051811:
2024-05-29 22:10:18: CHEMBL918634:
2024-05-29 22:10:19: CHEMBL3089774:
2024-05-29 22:10:20: CHEMBL2046046:
2024-05-29 22:10:21: CHEMBL934825:
2024-05-29 22:10:23: CHEMBL4008248:
2024-05-29 22:10:23: CHEMBL3243451:
2024-05-29 22:10:24: CHEMBL984606:
2024-05-29 22:10:25: CHEMBL814078:
2024-05-29 22:10:26: CHEMBL887635:
2024-05-29 22:10:27: CHEMBL1115306:
2024-05-29 22:10:28: CHEMBL3368957:
2024-05-29 22:10:29: CHEMBL4194448:
2024-05-29 22:10:30: CHEMBL1058849:
2024-05-29 22:10:30: CHEMBL659279:
2024-05-29 22:10:31: CHEMBL4012796:
2024-05-29 22:10:32: CHEMBL871030:
2024-05-29 22:10:33: CHEMBL2388192:
2024-05-29 22:10:34: CHEMBL1219014:
2024-05-29 22:10:35: CHEMBL3293254:
2024-05-29 22:10:36: CHEMBL710864:
2024-05-29 22:10:37: CHEMBL3999675:
2024-05-29 22:10:38: CHEMBL3807925:
2024-05-29 22:10:39: CHEMBL3060860:
2024-05-29 22:10:40: CHEMBL4039492:
2024-05-29 22:10:41: CHEMBL1614018:
2024-05-29 22:10:42: CHEMBL2379829:
2024-05-29 22:10:42: CHEMBL960357:
2024-05-29 22:10:43: CHEMBL3412552:
2024-05-29 22:10:44: CHEMBL914626:
2024-05-29 22:10:46: CHEMBL2154557:
2024-05-29 22:10:47: CHEMBL710381:
2024-05-29 22:10:48: CHEMBL1776383:
2024-05-29 22:10:50: CHEMBL1663702:
2024-05-29 22:10:53: CHEMBL907805:
2024-05-29 22:10:54: CHEMBL835015:
2024-05-29 22:10:57: CHEMBL3999676:
2024-05-29 22:10:58: CHEMBL924496:
2024-05-29 22:10:58: CHEMBL3062734:
2024-05-29 22:10:59: CHEMBL3123238:
2024-05-29 22:11:00: CHEMBL4721621:
2024-05-29 22:11:01: CHEMBL1033999:
2024-05-29 22:11:01: CHEMBL4276507:
