2024-05-05 09:24:03: Starting train run FSMol_ProtoNet_gnn+ecfp+fc_2024-05-05_09-24-03.
2024-05-05 09:24:03: 	Arguments: Namespace(DATA_PATH='/home/gridsan/ppaschalidis/Ersilia-FS/dataset/min_size_16', task_list_file='/home/gridsan/ppaschalidis/Ersilia-FS/dataset/min_size_16/entire_train_set.json', save_dir='/home/gridsan/ppaschalidis/Ersilia-FS/fs_mol/outputs/train/FSMol_ProtoNet_2024-05-03-support-16', seed=0, azureml_logging=False, features='gnn+ecfp+fc', distance_metric='mahalanobis', gnn_type='PNA', node_embed_dim=128, num_heads=4, per_head_dim=64, intermediate_dim=1024, message_function_depth=1, num_gnn_layers=10, readout_type='combined', readout_use_all_states=True, readout_num_heads=12, readout_head_dim=64, readout_output_dim=512, support_set_size=16, query_set_size=256, tasks_per_batch=16, batch_size=256, num_train_steps=2500, validate_every=100, validation_support_set_sizes=[16], validation_query_set_size=512, validation_num_samples=5, lr=10.0, clip_value=None, pretrained_gnn='/home/gridsan/ppaschalidis/Ersilia-FS/weights/multitask_best_model.pt', pretrained_pn=None)
2024-05-05 09:24:03: 	Output dir: /home/gridsan/ppaschalidis/Ersilia-FS/fs_mol/outputs/train/FSMol_ProtoNet_2024-05-03-support-16/FSMol_ProtoNet_gnn+ecfp+fc_2024-05-05_09-24-03
2024-05-05 09:24:03: 	Data path: /home/gridsan/ppaschalidis/Ersilia-FS/dataset/min_size_16
2024-05-05 09:24:03: Identified 655 training tasks.
2024-05-05 09:24:03: Identified 34 validation tasks.
2024-05-05 09:24:03: Identified 200 test tasks.
2024-05-05 09:24:03: 	Device: cpu
2024-05-05 09:24:03: 	Num parameters 19042338
2024-05-05 09:24:03: 	Model:
PrototypicalNetworkTrainer(
  (graph_feature_extractor): GraphFeatureExtractor(
    (init_node_proj): Linear(in_features=32, out_features=128, bias=False)
    (gnn): GNN(
      (gnn_blocks): ModuleList(
        (0): GNNBlock(
          (mp_layers): ModuleList(
            (0): RelationalMultiAggrMP(
              (message_fns): ModuleList(
                (0): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
                (1): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
                (2): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
              )
            )
            (1): RelationalMultiAggrMP(
              (message_fns): ModuleList(
                (0): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
                (1): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
                (2): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
              )
            )
            (2): RelationalMultiAggrMP(
              (message_fns): ModuleList(
                (0): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
                (1): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
                (2): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
              )
            )
            (3): RelationalMultiAggrMP(
              (message_fns): ModuleList(
                (0): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
                (1): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
                (2): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
              )
            )
          )
          (msg_out_projection): Linear(in_features=3072, out_features=128, bias=True)
          (mp_norm_layer): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (boom_layer): BOOMLayer(
            (linear1): Linear(in_features=128, out_features=1024, bias=True)
            (activation): LeakyReLU(negative_slope=0.01)
            (dropout): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=128, bias=True)
          )
          (boom_norm_layer): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (dropout_layer): Dropout(p=0.0, inplace=False)
        )
        (1): GNNBlock(
          (mp_layers): ModuleList(
            (0): RelationalMultiAggrMP(
              (message_fns): ModuleList(
                (0): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
                (1): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
                (2): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
              )
            )
            (1): RelationalMultiAggrMP(
              (message_fns): ModuleList(
                (0): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
                (1): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
                (2): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
              )
            )
            (2): RelationalMultiAggrMP(
              (message_fns): ModuleList(
                (0): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
                (1): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
                (2): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
              )
            )
            (3): RelationalMultiAggrMP(
              (message_fns): ModuleList(
                (0): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
                (1): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
                (2): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
              )
            )
          )
          (msg_out_projection): Linear(in_features=3072, out_features=128, bias=True)
          (mp_norm_layer): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (boom_layer): BOOMLayer(
            (linear1): Linear(in_features=128, out_features=1024, bias=True)
            (activation): LeakyReLU(negative_slope=0.01)
            (dropout): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=128, bias=True)
          )
          (boom_norm_layer): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (dropout_layer): Dropout(p=0.0, inplace=False)
        )
        (2): GNNBlock(
          (mp_layers): ModuleList(
            (0): RelationalMultiAggrMP(
              (message_fns): ModuleList(
                (0): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
                (1): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
                (2): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
              )
            )
            (1): RelationalMultiAggrMP(
              (message_fns): ModuleList(
                (0): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
                (1): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
                (2): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
              )
            )
            (2): RelationalMultiAggrMP(
              (message_fns): ModuleList(
                (0): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
                (1): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
                (2): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
              )
            )
            (3): RelationalMultiAggrMP(
              (message_fns): ModuleList(
                (0): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
                (1): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
                (2): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
              )
            )
          )
          (msg_out_projection): Linear(in_features=3072, out_features=128, bias=True)
          (mp_norm_layer): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (boom_layer): BOOMLayer(
            (linear1): Linear(in_features=128, out_features=1024, bias=True)
            (activation): LeakyReLU(negative_slope=0.01)
            (dropout): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=128, bias=True)
          )
          (boom_norm_layer): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (dropout_layer): Dropout(p=0.0, inplace=False)
        )
        (3): GNNBlock(
          (mp_layers): ModuleList(
            (0): RelationalMultiAggrMP(
              (message_fns): ModuleList(
                (0): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
                (1): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
                (2): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
              )
            )
            (1): RelationalMultiAggrMP(
              (message_fns): ModuleList(
                (0): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
                (1): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
                (2): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
              )
            )
            (2): RelationalMultiAggrMP(
              (message_fns): ModuleList(
                (0): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
                (1): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
                (2): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
              )
            )
            (3): RelationalMultiAggrMP(
              (message_fns): ModuleList(
                (0): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
                (1): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
                (2): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
              )
            )
          )
          (msg_out_projection): Linear(in_features=3072, out_features=128, bias=True)
          (mp_norm_layer): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (boom_layer): BOOMLayer(
            (linear1): Linear(in_features=128, out_features=1024, bias=True)
            (activation): LeakyReLU(negative_slope=0.01)
            (dropout): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=128, bias=True)
          )
          (boom_norm_layer): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (dropout_layer): Dropout(p=0.0, inplace=False)
        )
        (4): GNNBlock(
          (mp_layers): ModuleList(
            (0): RelationalMultiAggrMP(
              (message_fns): ModuleList(
                (0): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
                (1): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
                (2): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
              )
            )
            (1): RelationalMultiAggrMP(
              (message_fns): ModuleList(
                (0): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
                (1): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
                (2): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
              )
            )
            (2): RelationalMultiAggrMP(
              (message_fns): ModuleList(
                (0): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
                (1): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
                (2): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
              )
            )
            (3): RelationalMultiAggrMP(
              (message_fns): ModuleList(
                (0): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
                (1): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
                (2): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
              )
            )
          )
          (msg_out_projection): Linear(in_features=3072, out_features=128, bias=True)
          (mp_norm_layer): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (boom_layer): BOOMLayer(
            (linear1): Linear(in_features=128, out_features=1024, bias=True)
            (activation): LeakyReLU(negative_slope=0.01)
            (dropout): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=128, bias=True)
          )
          (boom_norm_layer): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (dropout_layer): Dropout(p=0.0, inplace=False)
        )
        (5): GNNBlock(
          (mp_layers): ModuleList(
            (0): RelationalMultiAggrMP(
              (message_fns): ModuleList(
                (0): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
                (1): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
                (2): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
              )
            )
            (1): RelationalMultiAggrMP(
              (message_fns): ModuleList(
                (0): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
                (1): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
                (2): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
              )
            )
            (2): RelationalMultiAggrMP(
              (message_fns): ModuleList(
                (0): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
                (1): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
                (2): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
              )
            )
            (3): RelationalMultiAggrMP(
              (message_fns): ModuleList(
                (0): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
                (1): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
                (2): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
              )
            )
          )
          (msg_out_projection): Linear(in_features=3072, out_features=128, bias=True)
          (mp_norm_layer): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (boom_layer): BOOMLayer(
            (linear1): Linear(in_features=128, out_features=1024, bias=True)
            (activation): LeakyReLU(negative_slope=0.01)
            (dropout): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=128, bias=True)
          )
          (boom_norm_layer): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (dropout_layer): Dropout(p=0.0, inplace=False)
        )
        (6): GNNBlock(
          (mp_layers): ModuleList(
            (0): RelationalMultiAggrMP(
              (message_fns): ModuleList(
                (0): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
                (1): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
                (2): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
              )
            )
            (1): RelationalMultiAggrMP(
              (message_fns): ModuleList(
                (0): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
                (1): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
                (2): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
              )
            )
            (2): RelationalMultiAggrMP(
              (message_fns): ModuleList(
                (0): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
                (1): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
                (2): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
              )
            )
            (3): RelationalMultiAggrMP(
              (message_fns): ModuleList(
                (0): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
                (1): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
                (2): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
              )
            )
          )
          (msg_out_projection): Linear(in_features=3072, out_features=128, bias=True)
          (mp_norm_layer): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (boom_layer): BOOMLayer(
            (linear1): Linear(in_features=128, out_features=1024, bias=True)
            (activation): LeakyReLU(negative_slope=0.01)
            (dropout): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=128, bias=True)
          )
          (boom_norm_layer): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (dropout_layer): Dropout(p=0.0, inplace=False)
        )
        (7): GNNBlock(
          (mp_layers): ModuleList(
            (0): RelationalMultiAggrMP(
              (message_fns): ModuleList(
                (0): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
                (1): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
                (2): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
              )
            )
            (1): RelationalMultiAggrMP(
              (message_fns): ModuleList(
                (0): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
                (1): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
                (2): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
              )
            )
            (2): RelationalMultiAggrMP(
              (message_fns): ModuleList(
                (0): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
                (1): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
                (2): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
              )
            )
            (3): RelationalMultiAggrMP(
              (message_fns): ModuleList(
                (0): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
                (1): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
                (2): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
              )
            )
          )
          (msg_out_projection): Linear(in_features=3072, out_features=128, bias=True)
          (mp_norm_layer): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (boom_layer): BOOMLayer(
            (linear1): Linear(in_features=128, out_features=1024, bias=True)
            (activation): LeakyReLU(negative_slope=0.01)
            (dropout): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=128, bias=True)
          )
          (boom_norm_layer): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (dropout_layer): Dropout(p=0.0, inplace=False)
        )
        (8): GNNBlock(
          (mp_layers): ModuleList(
            (0): RelationalMultiAggrMP(
              (message_fns): ModuleList(
                (0): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
                (1): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
                (2): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
              )
            )
            (1): RelationalMultiAggrMP(
              (message_fns): ModuleList(
                (0): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
                (1): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
                (2): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
              )
            )
            (2): RelationalMultiAggrMP(
              (message_fns): ModuleList(
                (0): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
                (1): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
                (2): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
              )
            )
            (3): RelationalMultiAggrMP(
              (message_fns): ModuleList(
                (0): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
                (1): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
                (2): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
              )
            )
          )
          (msg_out_projection): Linear(in_features=3072, out_features=128, bias=True)
          (mp_norm_layer): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (boom_layer): BOOMLayer(
            (linear1): Linear(in_features=128, out_features=1024, bias=True)
            (activation): LeakyReLU(negative_slope=0.01)
            (dropout): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=128, bias=True)
          )
          (boom_norm_layer): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (dropout_layer): Dropout(p=0.0, inplace=False)
        )
        (9): GNNBlock(
          (mp_layers): ModuleList(
            (0): RelationalMultiAggrMP(
              (message_fns): ModuleList(
                (0): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
                (1): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
                (2): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
              )
            )
            (1): RelationalMultiAggrMP(
              (message_fns): ModuleList(
                (0): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
                (1): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
                (2): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
              )
            )
            (2): RelationalMultiAggrMP(
              (message_fns): ModuleList(
                (0): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
                (1): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
                (2): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
              )
            )
            (3): RelationalMultiAggrMP(
              (message_fns): ModuleList(
                (0): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
                (1): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
                (2): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
              )
            )
          )
          (msg_out_projection): Linear(in_features=3072, out_features=128, bias=True)
          (mp_norm_layer): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (boom_layer): BOOMLayer(
            (linear1): Linear(in_features=128, out_features=1024, bias=True)
            (activation): LeakyReLU(negative_slope=0.01)
            (dropout): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=128, bias=True)
          )
          (boom_norm_layer): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (dropout_layer): Dropout(p=0.0, inplace=False)
        )
      )
    )
    (readout): CombinedGraphReadout(
      (_weighted_mean_pooler): MultiHeadWeightedGraphReadout(
        (_scoring_module): MLP(
          (_layers): Sequential(
            (0): Linear(in_features=1408, out_features=768, bias=True)
            (1): ReLU()
            (2): Linear(in_features=768, out_features=12, bias=True)
          )
        )
        (_transformation_mlp): MLP(
          (_layers): Sequential(
            (0): Linear(in_features=1408, out_features=768, bias=True)
            (1): ReLU()
            (2): Linear(in_features=768, out_features=768, bias=True)
          )
        )
        (_combination_layer): Linear(in_features=768, out_features=512, bias=False)
      )
      (_weighted_sum_pooler): MultiHeadWeightedGraphReadout(
        (_scoring_module): MLP(
          (_layers): Sequential(
            (0): Linear(in_features=1408, out_features=768, bias=True)
            (1): ReLU()
            (2): Linear(in_features=768, out_features=12, bias=True)
          )
        )
        (_transformation_mlp): MLP(
          (_layers): Sequential(
            (0): Linear(in_features=1408, out_features=768, bias=True)
            (1): ReLU()
            (2): Linear(in_features=768, out_features=768, bias=True)
          )
        )
        (_combination_layer): Linear(in_features=768, out_features=512, bias=False)
      )
      (_max_pooler): UnweightedGraphReadout(
        (_combination_layer): Linear(in_features=1408, out_features=512, bias=False)
      )
      (_combination_layer): Linear(in_features=1536, out_features=512, bias=False)
    )
  )
  (fc): Sequential(
    (0): Linear(in_features=2560, out_features=1024, bias=True)
    (1): ReLU()
    (2): Linear(in_features=1024, out_features=512, bias=True)
  )
)
2024-05-05 09:24:03: Loading pretrained GNN weights from /home/gridsan/ppaschalidis/Ersilia-FS/weights/multitask_best_model.pt.
2024-05-05 09:24:07: CHEMBL3285745:
2024-05-05 09:24:07: CHEMBL761987:
2024-05-05 09:24:07: CHEMBL1794398:
2024-05-05 09:24:07: CHEMBL703466:
2024-05-05 09:24:07: CHEMBL1259550:
2024-05-05 09:24:07: CHEMBL3227331:
2024-05-05 09:24:07: CHEMBL3761618:
2024-05-05 09:24:07: CHEMBL995355:
2024-05-05 09:24:07: CHEMBL1047854:
2024-05-05 09:24:07: CHEMBL3383460:
2024-05-05 09:24:07: CHEMBL695986:
2024-05-05 09:24:07: CHEMBL4197926:
2024-05-05 09:24:07: CHEMBL833990:
2024-05-05 09:24:07: CHEMBL2150628:
2024-05-05 09:24:07: CHEMBL2162371:
2024-05-05 09:24:07: CHEMBL1177528:
2024-05-05 09:24:07: CHEMBL863382:
2024-05-05 09:24:07: CHEMBL3625422:
2024-05-05 09:24:07: CHEMBL3588714:
2024-05-05 09:24:07: CHEMBL895182:
2024-05-05 09:24:07: CHEMBL4002825:
2024-05-05 09:24:07: CHEMBL1107214:
2024-05-05 09:24:07: CHEMBL1020690:
2024-05-05 09:24:07: CHEMBL4187321:
2024-05-05 09:24:07: CHEMBL1068293:
2024-05-05 09:24:07: CHEMBL930744:
2024-05-05 09:24:07: CHEMBL909968:
2024-05-05 09:24:07: CHEMBL1053167:
2024-05-05 09:24:07: CHEMBL1107997:
2024-05-05 09:24:07: CHEMBL804251:
2024-05-05 09:24:07: CHEMBL1941610:
2024-05-05 09:24:07: CHEMBL1767670:
2024-05-05 09:24:07: CHEMBL1055019:
2024-05-05 09:24:07: CHEMBL3389542:
2024-05-05 09:24:07: CHEMBL871033:
2024-05-05 09:24:07: CHEMBL956777:
2024-05-05 09:24:07: CHEMBL2388705:
2024-05-05 09:24:07: CHEMBL656397:
2024-05-05 09:24:07: CHEMBL4263092:
2024-05-05 09:24:07: CHEMBL4432407:
2024-05-05 09:24:07: CHEMBL3295662:
2024-05-05 09:24:07: CHEMBL3405593:
2024-05-05 09:24:07: CHEMBL1803360:
2024-05-05 09:24:07: CHEMBL2447603:
2024-05-05 09:24:07: CHEMBL2328533:
2024-05-05 09:24:07: CHEMBL1034018:
2024-05-05 09:24:07: CHEMBL3384578:
2024-05-05 09:24:07: CHEMBL4054609:
2024-05-05 09:24:07: CHEMBL833931:
2024-05-05 09:24:07: CHEMBL710534:
2024-05-05 09:24:07: CHEMBL1116600:
2024-05-05 09:24:07: CHEMBL2051603:
2024-05-05 09:24:07: CHEMBL860970:
2024-05-05 09:24:07: CHEMBL713255:
2024-05-05 09:24:07: CHEMBL1816524:
2024-05-05 09:24:07: CHEMBL3266569:
2024-05-05 09:24:07: CHEMBL922474:
2024-05-05 09:24:07: CHEMBL941164:
2024-05-05 09:24:07: CHEMBL1121169:
2024-05-05 09:24:07: CHEMBL2024927:
2024-05-05 09:24:07: CHEMBL1960697:
2024-05-05 09:24:07: CHEMBL3367829:
2024-05-05 09:24:07: CHEMBL937510:
2024-05-05 09:24:07: CHEMBL1051810:
2024-05-05 09:24:07: CHEMBL2405783:
2024-05-05 09:24:07: CHEMBL3396060:
2024-05-05 09:24:07: CHEMBL708445:
2024-05-05 09:24:07: CHEMBL4136409:
2024-05-05 09:24:07: CHEMBL2051598:
2024-05-05 09:24:07: CHEMBL4194391:
2024-05-05 09:24:07: CHEMBL1763053:
2024-05-05 09:24:07: CHEMBL2067378:
2024-05-05 09:24:07: CHEMBL826375:
2024-05-05 09:24:07: CHEMBL889770:
2024-05-05 09:24:07: CHEMBL903139:
2024-05-05 09:24:07: CHEMBL3240111:
2024-05-05 09:24:07: CHEMBL2033222:
2024-05-05 09:24:07: CHEMBL712878:
2024-05-05 09:24:07: CHEMBL713409:
2024-05-05 09:24:07: CHEMBL3295663:
2024-05-05 09:24:07: CHEMBL1815005:
2024-05-05 09:24:07: CHEMBL1018957:
2024-05-05 09:24:07: CHEMBL2148600:
2024-05-05 09:24:07: CHEMBL1228542:
2024-05-05 09:24:07: CHEMBL1654782:
2024-05-05 09:24:07: CHEMBL2072118:
2024-05-05 09:24:07: CHEMBL1260089:
2024-05-05 09:24:07: CHEMBL4719106:
2024-05-05 09:24:07: CHEMBL3067105:
2024-05-05 09:24:07: CHEMBL1654780:
2024-05-05 09:24:07: CHEMBL2019245:
2024-05-05 09:24:07: CHEMBL961554:
2024-05-05 09:24:07: CHEMBL2341227:
2024-05-05 09:24:07: CHEMBL2154555:
2024-05-05 09:24:07: CHEMBL899618:
2024-05-05 09:24:07: CHEMBL1219015:
2024-05-05 09:24:07: CHEMBL902229:
2024-05-05 09:24:07: CHEMBL833275:
2024-05-05 09:24:07: CHEMBL3413997:
2024-05-05 09:24:07: CHEMBL3578893:
2024-05-05 09:24:07: CHEMBL3418331:
2024-05-05 09:24:07: CHEMBL2445761:
2024-05-05 09:24:07: CHEMBL833792:
2024-05-05 09:24:07: CHEMBL1053306:
2024-05-05 09:24:07: CHEMBL1017067:
2024-05-05 09:24:07: CHEMBL1033996:
2024-05-05 09:24:07: CHEMBL889768:
2024-05-05 09:24:07: CHEMBL2405555:
2024-05-05 09:24:07: CHEMBL3390310:
2024-05-05 09:24:07: CHEMBL1052565:
2024-05-05 09:24:07: CHEMBL1932094:
2024-05-05 09:24:07: CHEMBL2379584:
2024-05-05 09:24:07: CHEMBL689145:
2024-05-05 09:24:07: CHEMBL2210037:
2024-05-05 09:24:07: CHEMBL984423:
2024-05-05 09:24:07: CHEMBL1930242:
2024-05-05 09:24:07: CHEMBL4253251:
2024-05-05 09:24:07: CHEMBL833117:
2024-05-05 09:24:07: CHEMBL1817498:
2024-05-05 09:24:07: CHEMBL1039164:
2024-05-05 09:24:07: CHEMBL1837558:
2024-05-05 09:24:07: CHEMBL906490:
2024-05-05 09:24:07: CHEMBL3073615:
2024-05-05 09:24:07: CHEMBL908714:
2024-05-05 09:24:07: CHEMBL995360:
2024-05-05 09:24:07: CHEMBL1056891:
2024-05-05 09:24:07: CHEMBL1953907:
2024-05-05 09:24:07: CHEMBL899467:
2024-05-05 09:24:07: CHEMBL1794318:
2024-05-05 09:24:07: CHEMBL833080:
2024-05-05 09:24:07: CHEMBL936680:
2024-05-05 09:24:07: CHEMBL1024879:
2024-05-05 09:24:07: CHEMBL3122259:
2024-05-05 09:24:08: CHEMBL2342597:
2024-05-05 09:24:08: CHEMBL3282588:
2024-05-05 09:24:09: CHEMBL3078730:
2024-05-05 09:24:09: CHEMBL860972:
2024-05-05 09:24:10: CHEMBL3295321:
2024-05-05 09:24:10: CHEMBL3815782:
2024-05-05 09:24:11: CHEMBL3088595:
2024-05-05 09:24:12: CHEMBL655136:
2024-05-05 09:24:13: CHEMBL3418917:
2024-05-05 09:24:14: CHEMBL924692:
2024-05-05 09:24:15: CHEMBL713254:
2024-05-05 09:24:15: CHEMBL1024326:
2024-05-05 09:24:16: CHEMBL1059735:
2024-05-05 09:24:17: CHEMBL683837:
2024-05-05 09:24:17: CHEMBL1816523:
2024-05-05 09:24:18: CHEMBL3129213:
2024-05-05 09:24:18: CHEMBL3077285:
2024-05-05 09:24:19: CHEMBL4477287:
2024-05-05 09:24:20: CHEMBL2390255:
2024-05-05 09:24:21: CHEMBL711656:
2024-05-05 09:24:21: CHEMBL3295320:
2024-05-05 09:24:22: CHEMBL758925:
2024-05-05 09:24:23: CHEMBL1035506:
2024-05-05 09:24:23: CHEMBL1054762:
2024-05-05 09:24:24: CHEMBL906554:
2024-05-05 09:24:25: CHEMBL1921081:
2024-05-05 09:24:25: CHEMBL3072372:
