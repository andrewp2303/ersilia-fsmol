2024-05-04 23:17:51: Starting train run FSMol_ProtoNet_gnn+ecfp+fc_2024-05-04_23-17-51.
2024-05-04 23:17:51: 	Arguments: Namespace(DATA_PATH='/home/gridsan/ppaschalidis/Ersilia-FS/dataset/min_size_16', task_list_file='/home/gridsan/ppaschalidis/Ersilia-FS/dataset/min_size_16/entire_train_set.json', save_dir='/home/gridsan/ppaschalidis/Ersilia-FS/fs_mol/outputs/train/FSMol_ProtoNet_2024-05-03-support-16', seed=0, azureml_logging=False, features='gnn+ecfp+fc', distance_metric='mahalanobis', gnn_type='PNA', node_embed_dim=128, num_heads=4, per_head_dim=64, intermediate_dim=1024, message_function_depth=1, num_gnn_layers=10, readout_type='combined', readout_use_all_states=True, readout_num_heads=12, readout_head_dim=64, readout_output_dim=512, support_set_size=16, query_set_size=256, tasks_per_batch=64, batch_size=256, num_train_steps=2500, validate_every=100, validation_support_set_sizes=[16], validation_query_set_size=512, validation_num_samples=5, lr=1.0, clip_value=None, pretrained_gnn='/home/gridsan/ppaschalidis/Ersilia-FS/weights/multitask_best_model.pt', pretrained_pn=None)
2024-05-04 23:17:51: 	Output dir: /home/gridsan/ppaschalidis/Ersilia-FS/fs_mol/outputs/train/FSMol_ProtoNet_2024-05-03-support-16/FSMol_ProtoNet_gnn+ecfp+fc_2024-05-04_23-17-51
2024-05-04 23:17:51: 	Data path: /home/gridsan/ppaschalidis/Ersilia-FS/dataset/min_size_16
2024-05-04 23:17:51: Identified 655 training tasks.
2024-05-04 23:17:51: Identified 34 validation tasks.
2024-05-04 23:17:51: Identified 200 test tasks.
2024-05-04 23:17:52: 	Device: cpu
2024-05-04 23:17:52: 	Num parameters 19042338
2024-05-04 23:17:52: 	Model:
PrototypicalNetworkTrainer(
  (graph_feature_extractor): GraphFeatureExtractor(
    (init_node_proj): Linear(in_features=32, out_features=128, bias=False)
    (gnn): GNN(
      (gnn_blocks): ModuleList(
        (0): GNNBlock(
          (mp_layers): ModuleList(
            (0): RelationalMultiAggrMP(
              (message_fns): ModuleList(
                (0): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
                (1): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
                (2): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
              )
            )
            (1): RelationalMultiAggrMP(
              (message_fns): ModuleList(
                (0): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
                (1): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
                (2): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
              )
            )
            (2): RelationalMultiAggrMP(
              (message_fns): ModuleList(
                (0): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
                (1): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
                (2): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
              )
            )
            (3): RelationalMultiAggrMP(
              (message_fns): ModuleList(
                (0): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
                (1): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
                (2): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
              )
            )
          )
          (msg_out_projection): Linear(in_features=3072, out_features=128, bias=True)
          (mp_norm_layer): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (boom_layer): BOOMLayer(
            (linear1): Linear(in_features=128, out_features=1024, bias=True)
            (activation): LeakyReLU(negative_slope=0.01)
            (dropout): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=128, bias=True)
          )
          (boom_norm_layer): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (dropout_layer): Dropout(p=0.0, inplace=False)
        )
        (1): GNNBlock(
          (mp_layers): ModuleList(
            (0): RelationalMultiAggrMP(
              (message_fns): ModuleList(
                (0): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
                (1): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
                (2): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
              )
            )
            (1): RelationalMultiAggrMP(
              (message_fns): ModuleList(
                (0): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
                (1): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
                (2): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
              )
            )
            (2): RelationalMultiAggrMP(
              (message_fns): ModuleList(
                (0): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
                (1): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
                (2): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
              )
            )
            (3): RelationalMultiAggrMP(
              (message_fns): ModuleList(
                (0): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
                (1): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
                (2): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
              )
            )
          )
          (msg_out_projection): Linear(in_features=3072, out_features=128, bias=True)
          (mp_norm_layer): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (boom_layer): BOOMLayer(
            (linear1): Linear(in_features=128, out_features=1024, bias=True)
            (activation): LeakyReLU(negative_slope=0.01)
            (dropout): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=128, bias=True)
          )
          (boom_norm_layer): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (dropout_layer): Dropout(p=0.0, inplace=False)
        )
        (2): GNNBlock(
          (mp_layers): ModuleList(
            (0): RelationalMultiAggrMP(
              (message_fns): ModuleList(
                (0): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
                (1): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
                (2): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
              )
            )
            (1): RelationalMultiAggrMP(
              (message_fns): ModuleList(
                (0): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
                (1): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
                (2): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
              )
            )
            (2): RelationalMultiAggrMP(
              (message_fns): ModuleList(
                (0): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
                (1): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
                (2): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
              )
            )
            (3): RelationalMultiAggrMP(
              (message_fns): ModuleList(
                (0): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
                (1): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
                (2): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
              )
            )
          )
          (msg_out_projection): Linear(in_features=3072, out_features=128, bias=True)
          (mp_norm_layer): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (boom_layer): BOOMLayer(
            (linear1): Linear(in_features=128, out_features=1024, bias=True)
            (activation): LeakyReLU(negative_slope=0.01)
            (dropout): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=128, bias=True)
          )
          (boom_norm_layer): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (dropout_layer): Dropout(p=0.0, inplace=False)
        )
        (3): GNNBlock(
          (mp_layers): ModuleList(
            (0): RelationalMultiAggrMP(
              (message_fns): ModuleList(
                (0): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
                (1): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
                (2): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
              )
            )
            (1): RelationalMultiAggrMP(
              (message_fns): ModuleList(
                (0): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
                (1): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
                (2): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
              )
            )
            (2): RelationalMultiAggrMP(
              (message_fns): ModuleList(
                (0): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
                (1): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
                (2): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
              )
            )
            (3): RelationalMultiAggrMP(
              (message_fns): ModuleList(
                (0): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
                (1): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
                (2): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
              )
            )
          )
          (msg_out_projection): Linear(in_features=3072, out_features=128, bias=True)
          (mp_norm_layer): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (boom_layer): BOOMLayer(
            (linear1): Linear(in_features=128, out_features=1024, bias=True)
            (activation): LeakyReLU(negative_slope=0.01)
            (dropout): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=128, bias=True)
          )
          (boom_norm_layer): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (dropout_layer): Dropout(p=0.0, inplace=False)
        )
        (4): GNNBlock(
          (mp_layers): ModuleList(
            (0): RelationalMultiAggrMP(
              (message_fns): ModuleList(
                (0): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
                (1): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
                (2): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
              )
            )
            (1): RelationalMultiAggrMP(
              (message_fns): ModuleList(
                (0): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
                (1): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
                (2): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
              )
            )
            (2): RelationalMultiAggrMP(
              (message_fns): ModuleList(
                (0): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
                (1): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
                (2): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
              )
            )
            (3): RelationalMultiAggrMP(
              (message_fns): ModuleList(
                (0): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
                (1): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
                (2): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
              )
            )
          )
          (msg_out_projection): Linear(in_features=3072, out_features=128, bias=True)
          (mp_norm_layer): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (boom_layer): BOOMLayer(
            (linear1): Linear(in_features=128, out_features=1024, bias=True)
            (activation): LeakyReLU(negative_slope=0.01)
            (dropout): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=128, bias=True)
          )
          (boom_norm_layer): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (dropout_layer): Dropout(p=0.0, inplace=False)
        )
        (5): GNNBlock(
          (mp_layers): ModuleList(
            (0): RelationalMultiAggrMP(
              (message_fns): ModuleList(
                (0): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
                (1): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
                (2): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
              )
            )
            (1): RelationalMultiAggrMP(
              (message_fns): ModuleList(
                (0): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
                (1): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
                (2): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
              )
            )
            (2): RelationalMultiAggrMP(
              (message_fns): ModuleList(
                (0): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
                (1): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
                (2): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
              )
            )
            (3): RelationalMultiAggrMP(
              (message_fns): ModuleList(
                (0): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
                (1): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
                (2): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
              )
            )
          )
          (msg_out_projection): Linear(in_features=3072, out_features=128, bias=True)
          (mp_norm_layer): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (boom_layer): BOOMLayer(
            (linear1): Linear(in_features=128, out_features=1024, bias=True)
            (activation): LeakyReLU(negative_slope=0.01)
            (dropout): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=128, bias=True)
          )
          (boom_norm_layer): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (dropout_layer): Dropout(p=0.0, inplace=False)
        )
        (6): GNNBlock(
          (mp_layers): ModuleList(
            (0): RelationalMultiAggrMP(
              (message_fns): ModuleList(
                (0): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
                (1): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
                (2): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
              )
            )
            (1): RelationalMultiAggrMP(
              (message_fns): ModuleList(
                (0): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
                (1): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
                (2): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
              )
            )
            (2): RelationalMultiAggrMP(
              (message_fns): ModuleList(
                (0): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
                (1): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
                (2): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
              )
            )
            (3): RelationalMultiAggrMP(
              (message_fns): ModuleList(
                (0): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
                (1): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
                (2): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
              )
            )
          )
          (msg_out_projection): Linear(in_features=3072, out_features=128, bias=True)
          (mp_norm_layer): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (boom_layer): BOOMLayer(
            (linear1): Linear(in_features=128, out_features=1024, bias=True)
            (activation): LeakyReLU(negative_slope=0.01)
            (dropout): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=128, bias=True)
          )
          (boom_norm_layer): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (dropout_layer): Dropout(p=0.0, inplace=False)
        )
        (7): GNNBlock(
          (mp_layers): ModuleList(
            (0): RelationalMultiAggrMP(
              (message_fns): ModuleList(
                (0): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
                (1): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
                (2): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
              )
            )
            (1): RelationalMultiAggrMP(
              (message_fns): ModuleList(
                (0): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
                (1): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
                (2): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
              )
            )
            (2): RelationalMultiAggrMP(
              (message_fns): ModuleList(
                (0): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
                (1): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
                (2): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
              )
            )
            (3): RelationalMultiAggrMP(
              (message_fns): ModuleList(
                (0): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
                (1): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
                (2): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
              )
            )
          )
          (msg_out_projection): Linear(in_features=3072, out_features=128, bias=True)
          (mp_norm_layer): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (boom_layer): BOOMLayer(
            (linear1): Linear(in_features=128, out_features=1024, bias=True)
            (activation): LeakyReLU(negative_slope=0.01)
            (dropout): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=128, bias=True)
          )
          (boom_norm_layer): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (dropout_layer): Dropout(p=0.0, inplace=False)
        )
        (8): GNNBlock(
          (mp_layers): ModuleList(
            (0): RelationalMultiAggrMP(
              (message_fns): ModuleList(
                (0): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
                (1): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
                (2): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
              )
            )
            (1): RelationalMultiAggrMP(
              (message_fns): ModuleList(
                (0): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
                (1): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
                (2): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
              )
            )
            (2): RelationalMultiAggrMP(
              (message_fns): ModuleList(
                (0): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
                (1): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
                (2): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
              )
            )
            (3): RelationalMultiAggrMP(
              (message_fns): ModuleList(
                (0): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
                (1): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
                (2): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
              )
            )
          )
          (msg_out_projection): Linear(in_features=3072, out_features=128, bias=True)
          (mp_norm_layer): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (boom_layer): BOOMLayer(
            (linear1): Linear(in_features=128, out_features=1024, bias=True)
            (activation): LeakyReLU(negative_slope=0.01)
            (dropout): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=128, bias=True)
          )
          (boom_norm_layer): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (dropout_layer): Dropout(p=0.0, inplace=False)
        )
        (9): GNNBlock(
          (mp_layers): ModuleList(
            (0): RelationalMultiAggrMP(
              (message_fns): ModuleList(
                (0): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
                (1): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
                (2): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
              )
            )
            (1): RelationalMultiAggrMP(
              (message_fns): ModuleList(
                (0): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
                (1): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
                (2): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
              )
            )
            (2): RelationalMultiAggrMP(
              (message_fns): ModuleList(
                (0): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
                (1): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
                (2): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
              )
            )
            (3): RelationalMultiAggrMP(
              (message_fns): ModuleList(
                (0): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
                (1): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
                (2): MLP(
                  (_layers): Sequential(
                    (0): Linear(in_features=64, out_features=192, bias=True)
                  )
                )
              )
            )
          )
          (msg_out_projection): Linear(in_features=3072, out_features=128, bias=True)
          (mp_norm_layer): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (boom_layer): BOOMLayer(
            (linear1): Linear(in_features=128, out_features=1024, bias=True)
            (activation): LeakyReLU(negative_slope=0.01)
            (dropout): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=128, bias=True)
          )
          (boom_norm_layer): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (dropout_layer): Dropout(p=0.0, inplace=False)
        )
      )
    )
    (readout): CombinedGraphReadout(
      (_weighted_mean_pooler): MultiHeadWeightedGraphReadout(
        (_scoring_module): MLP(
          (_layers): Sequential(
            (0): Linear(in_features=1408, out_features=768, bias=True)
            (1): ReLU()
            (2): Linear(in_features=768, out_features=12, bias=True)
          )
        )
        (_transformation_mlp): MLP(
          (_layers): Sequential(
            (0): Linear(in_features=1408, out_features=768, bias=True)
            (1): ReLU()
            (2): Linear(in_features=768, out_features=768, bias=True)
          )
        )
        (_combination_layer): Linear(in_features=768, out_features=512, bias=False)
      )
      (_weighted_sum_pooler): MultiHeadWeightedGraphReadout(
        (_scoring_module): MLP(
          (_layers): Sequential(
            (0): Linear(in_features=1408, out_features=768, bias=True)
            (1): ReLU()
            (2): Linear(in_features=768, out_features=12, bias=True)
          )
        )
        (_transformation_mlp): MLP(
          (_layers): Sequential(
            (0): Linear(in_features=1408, out_features=768, bias=True)
            (1): ReLU()
            (2): Linear(in_features=768, out_features=768, bias=True)
          )
        )
        (_combination_layer): Linear(in_features=768, out_features=512, bias=False)
      )
      (_max_pooler): UnweightedGraphReadout(
        (_combination_layer): Linear(in_features=1408, out_features=512, bias=False)
      )
      (_combination_layer): Linear(in_features=1536, out_features=512, bias=False)
    )
  )
  (fc): Sequential(
    (0): Linear(in_features=2560, out_features=1024, bias=True)
    (1): ReLU()
    (2): Linear(in_features=1024, out_features=512, bias=True)
  )
)
2024-05-04 23:17:52: Loading pretrained GNN weights from /home/gridsan/ppaschalidis/Ersilia-FS/weights/multitask_best_model.pt.
2024-05-04 23:17:52: CHEMBL3285745:
2024-05-04 23:17:52: CHEMBL761987:
2024-05-04 23:17:52: CHEMBL1794398:
2024-05-04 23:17:52: CHEMBL703466:
2024-05-04 23:17:52: CHEMBL1259550:
2024-05-04 23:17:52: CHEMBL3227331:
2024-05-04 23:17:52: CHEMBL995355:
2024-05-04 23:17:52: CHEMBL3761618:
2024-05-04 23:17:52: CHEMBL3383460:
2024-05-04 23:17:52: CHEMBL1047854:
2024-05-04 23:17:52: CHEMBL833990:
2024-05-04 23:17:52: CHEMBL695986:
2024-05-04 23:17:52: CHEMBL4197926:
2024-05-04 23:17:52: CHEMBL3625422:
2024-05-04 23:17:52: CHEMBL863382:
2024-05-04 23:17:52: CHEMBL2150628:
2024-05-04 23:17:52: CHEMBL2162371:
2024-05-04 23:17:52: CHEMBL3588714:
2024-05-04 23:17:52: CHEMBL1177528:
2024-05-04 23:17:52: CHEMBL1020690:
2024-05-04 23:17:52: CHEMBL1107214:
2024-05-04 23:17:52: CHEMBL4002825:
2024-05-04 23:17:52: CHEMBL895182:
2024-05-04 23:17:52: CHEMBL930744:
2024-05-04 23:17:52: CHEMBL4187321:
2024-05-04 23:17:52: CHEMBL1068293:
2024-05-04 23:17:52: CHEMBL1053167:
2024-05-04 23:17:52: CHEMBL909968:
2024-05-04 23:17:52: CHEMBL804251:
2024-05-04 23:17:52: CHEMBL1107997:
2024-05-04 23:17:52: CHEMBL1941610:
2024-05-04 23:17:52: CHEMBL871033:
2024-05-04 23:17:52: CHEMBL1055019:
2024-05-04 23:17:52: CHEMBL3389542:
2024-05-04 23:17:52: CHEMBL956777:
2024-05-04 23:17:52: CHEMBL1767670:
2024-05-04 23:17:52: CHEMBL656397:
2024-05-04 23:17:52: CHEMBL4263092:
2024-05-04 23:17:52: CHEMBL4432407:
2024-05-04 23:17:52: CHEMBL2388705:
2024-05-04 23:17:52: CHEMBL2328533:
2024-05-04 23:17:52: CHEMBL3295662:
2024-05-04 23:17:52: CHEMBL3405593:
2024-05-04 23:17:52: CHEMBL3384578:
2024-05-04 23:17:52: CHEMBL1803360:
2024-05-04 23:17:52: CHEMBL1034018:
2024-05-04 23:17:52: CHEMBL2447603:
2024-05-04 23:17:52: CHEMBL710534:
2024-05-04 23:17:52: CHEMBL4054609:
2024-05-04 23:17:52: CHEMBL1116600:
2024-05-04 23:17:52: CHEMBL833931:
2024-05-04 23:17:52: CHEMBL2051603:
2024-05-04 23:17:52: CHEMBL860970:
2024-05-04 23:17:52: CHEMBL1816524:
2024-05-04 23:17:52: CHEMBL713255:
2024-05-04 23:17:52: CHEMBL3266569:
2024-05-04 23:17:52: CHEMBL922474:
2024-05-04 23:17:52: CHEMBL941164:
2024-05-04 23:17:52: CHEMBL1121169:
2024-05-04 23:17:52: CHEMBL2024927:
2024-05-04 23:17:52: CHEMBL1960697:
2024-05-04 23:17:52: CHEMBL3367829:
2024-05-04 23:17:52: CHEMBL937510:
2024-05-04 23:17:52: CHEMBL1051810:
2024-05-04 23:17:52: CHEMBL3396060:
2024-05-04 23:17:52: CHEMBL2405783:
2024-05-04 23:17:52: CHEMBL708445:
2024-05-04 23:17:52: CHEMBL4136409:
2024-05-04 23:17:52: CHEMBL2051598:
2024-05-04 23:17:52: CHEMBL4194391:
2024-05-04 23:17:52: CHEMBL1763053:
2024-05-04 23:17:52: CHEMBL2067378:
2024-05-04 23:17:52: CHEMBL826375:
2024-05-04 23:17:52: CHEMBL903139:
2024-05-04 23:17:52: CHEMBL3240111:
2024-05-04 23:17:52: CHEMBL2033222:
2024-05-04 23:17:52: CHEMBL712878:
2024-05-04 23:17:52: CHEMBL889770:
2024-05-04 23:17:52: CHEMBL713409:
2024-05-04 23:17:52: CHEMBL1815005:
2024-05-04 23:17:52: CHEMBL2148600:
2024-05-04 23:17:52: CHEMBL1018957:
2024-05-04 23:17:52: CHEMBL1228542:
2024-05-04 23:17:52: CHEMBL3295663:
2024-05-04 23:17:52: CHEMBL2072118:
2024-05-04 23:17:52: CHEMBL1654782:
2024-05-04 23:17:52: CHEMBL1260089:
2024-05-04 23:17:52: CHEMBL4719106:
2024-05-04 23:17:52: CHEMBL3067105:
2024-05-04 23:17:52: CHEMBL1654780:
2024-05-04 23:17:52: CHEMBL2019245:
2024-05-04 23:17:52: CHEMBL961554:
2024-05-04 23:17:52: CHEMBL2341227:
2024-05-04 23:17:52: CHEMBL2154555:
2024-05-04 23:17:52: CHEMBL899618:
2024-05-04 23:17:52: CHEMBL1219015:
2024-05-04 23:17:52: CHEMBL902229:
2024-05-04 23:17:52: CHEMBL833275:
2024-05-04 23:17:52: CHEMBL3413997:
2024-05-04 23:17:52: CHEMBL3578893:
2024-05-04 23:17:52: CHEMBL3418331:
2024-05-04 23:17:52: CHEMBL2445761:
2024-05-04 23:17:52: CHEMBL833792:
2024-05-04 23:17:52: CHEMBL1053306:
2024-05-04 23:17:52: CHEMBL1017067:
2024-05-04 23:17:53: CHEMBL1033996:
2024-05-04 23:17:53: CHEMBL889768:
2024-05-04 23:17:53: CHEMBL2405555:
2024-05-04 23:17:53: CHEMBL1052565:
2024-05-04 23:17:53: CHEMBL3390310:
2024-05-04 23:17:53: CHEMBL1932094:
2024-05-04 23:17:53: CHEMBL2379584:
2024-05-04 23:17:53: CHEMBL689145:
2024-05-04 23:17:53: CHEMBL984423:
2024-05-04 23:17:53: CHEMBL2210037:
2024-05-04 23:17:53: CHEMBL1930242:
2024-05-04 23:17:53: CHEMBL4253251:
2024-05-04 23:17:53: CHEMBL833117:
2024-05-04 23:17:53: CHEMBL1817498:
2024-05-04 23:17:53: CHEMBL1039164:
2024-05-04 23:17:53: CHEMBL1837558:
2024-05-04 23:17:53: CHEMBL906490:
2024-05-04 23:17:53: CHEMBL3073615:
2024-05-04 23:17:53: CHEMBL908714:
2024-05-04 23:17:53: CHEMBL995360:
2024-05-04 23:17:53: CHEMBL1056891:
2024-05-04 23:17:53: CHEMBL899467:
2024-05-04 23:17:53: CHEMBL1953907:
2024-05-04 23:17:53: CHEMBL1794318:
2024-05-04 23:17:53: CHEMBL833080:
2024-05-04 23:17:53: CHEMBL936680:
2024-05-04 23:17:53: CHEMBL1024879:
2024-05-04 23:17:53: CHEMBL3122259:
2024-05-04 23:17:54: CHEMBL2342597:
2024-05-04 23:17:54: CHEMBL3282588:
2024-05-04 23:17:54: CHEMBL3078730:
2024-05-04 23:17:55: CHEMBL860972:
2024-05-04 23:17:55: CHEMBL3295321:
2024-05-04 23:17:56: CHEMBL3815782:
2024-05-04 23:17:57: CHEMBL3088595:
2024-05-04 23:17:57: CHEMBL655136:
2024-05-04 23:17:58: CHEMBL3418917:
2024-05-04 23:17:59: CHEMBL924692:
2024-05-04 23:18:00: CHEMBL713254:
2024-05-04 23:18:00: CHEMBL1024326:
2024-05-04 23:18:01: CHEMBL1059735:
2024-05-04 23:18:02: CHEMBL683837:
2024-05-04 23:18:03: CHEMBL1816523:
2024-05-04 23:18:03: CHEMBL3129213:
2024-05-04 23:18:04: CHEMBL3077285:
2024-05-04 23:18:05: CHEMBL4477287:
2024-05-04 23:18:06: CHEMBL2390255:
2024-05-04 23:18:06: CHEMBL711656:
2024-05-04 23:18:07: CHEMBL3295320:
2024-05-04 23:18:08: CHEMBL758925:
2024-05-04 23:18:08: CHEMBL1035506:
2024-05-04 23:18:09: CHEMBL1054762:
2024-05-04 23:18:10: CHEMBL906554:
2024-05-04 23:18:11: CHEMBL1921081:
2024-05-04 23:18:11: CHEMBL3072372:
2024-05-04 23:18:12: CHEMBL698789:
2024-05-04 23:18:13: CHEMBL1109063:
2024-05-04 23:18:14: CHEMBL1073156:
2024-05-04 23:18:14: CHEMBL972376:
2024-05-04 23:18:14: CHEMBL2432161:
2024-05-04 23:18:15: CHEMBL1035508:
2024-05-04 23:18:15: CHEMBL2167912:
2024-05-04 23:18:17: CHEMBL3418701:
2024-05-04 23:18:17: CHEMBL1034017:
2024-05-04 23:18:18: CHEMBL2317095:
2024-05-04 23:18:18: CHEMBL1067060:
2024-05-04 23:18:19: CHEMBL2156781:
2024-05-04 23:18:21: CHEMBL1037279:
2024-05-04 23:18:21: CHEMBL711804:
2024-05-04 23:18:22: CHEMBL4006616:
2024-05-04 23:18:23: CHEMBL944781:
2024-05-04 23:18:23: CHEMBL833378:
2024-05-04 23:18:24: CHEMBL1663615:
2024-05-04 23:18:25: CHEMBL3295870:
2024-05-04 23:18:26: CHEMBL2183320:
2024-05-04 23:18:27: CHEMBL3122349:
2024-05-04 23:18:27: CHEMBL873604:
2024-05-04 23:18:28: CHEMBL1817497:
2024-05-04 23:18:29: CHEMBL944749:
2024-05-04 23:18:29: CHEMBL4220813:
2024-05-04 23:18:30: CHEMBL4002824:
2024-05-04 23:18:31: CHEMBL3592342:
2024-05-04 23:18:33: CHEMBL3129215:
2024-05-04 23:18:33: CHEMBL1918205:
2024-05-04 23:18:34: CHEMBL1006568:
2024-05-04 23:18:34: CHEMBL1918204:
2024-05-04 23:18:35: CHEMBL3123043:
2024-05-04 23:18:36: CHEMBL4179593:
2024-05-04 23:18:37: CHEMBL1045728:
2024-05-04 23:18:37: CHEMBL837535:
2024-05-04 23:18:38: CHEMBL4184680:
2024-05-04 23:18:42: CHEMBL955134:
2024-05-04 23:18:42: CHEMBL1769611:
2024-05-04 23:18:43: CHEMBL4050758:
2024-05-04 23:18:44: CHEMBL3994798:
2024-05-04 23:18:45: CHEMBL706945:
2024-05-04 23:18:45: CHEMBL2390645:
2024-05-04 23:18:46: CHEMBL907807:
2024-05-04 23:18:47: CHEMBL956847:
2024-05-04 23:18:47: CHEMBL1043900:
2024-05-04 23:18:49: CHEMBL946064:
2024-05-04 23:18:49: CHEMBL4264727:
2024-05-04 23:18:50: CHEMBL933048:
2024-05-04 23:18:51: CHEMBL1936129:
2024-05-04 23:18:52: CHEMBL1008279:
2024-05-04 23:18:53: CHEMBL863014:
2024-05-04 23:18:54: CHEMBL713546:
2024-05-04 23:18:55: CHEMBL922937:
2024-05-04 23:18:55: CHEMBL4276506:
2024-05-04 23:18:56: CHEMBL897415:
2024-05-04 23:18:57: CHEMBL968900:
2024-05-04 23:18:57: CHEMBL1832560:
2024-05-04 23:18:58: CHEMBL697276:
2024-05-04 23:18:59: CHEMBL833076:
2024-05-04 23:18:59: CHEMBL691343:
2024-05-04 23:19:00: CHEMBL1047137:
2024-05-04 23:19:01: CHEMBL984427:
2024-05-04 23:19:02: CHEMBL3096264:
2024-05-04 23:19:02: CHEMBL3122629:
2024-05-04 23:19:03: CHEMBL4019375:
2024-05-04 23:19:04: CHEMBL1664688:
2024-05-04 23:19:04: CHEMBL833855:
2024-05-04 23:19:05: CHEMBL759088:
2024-05-04 23:19:05: CHEMBL1051811:
2024-05-04 23:19:06: CHEMBL918634:
2024-05-04 23:19:06: CHEMBL3089774:
2024-05-04 23:19:08: CHEMBL2046046:
2024-05-04 23:19:08: CHEMBL934825:
2024-05-04 23:19:09: CHEMBL4008248:
2024-05-04 23:19:09: CHEMBL3243451:
2024-05-04 23:19:10: CHEMBL984606:
2024-05-04 23:19:11: CHEMBL814078:
2024-05-04 23:19:12: CHEMBL887635:
2024-05-04 23:19:13: CHEMBL1115306:
2024-05-04 23:19:13: CHEMBL3368957:
2024-05-04 23:19:15: CHEMBL4194448:
2024-05-04 23:19:15: CHEMBL1058849:
2024-05-04 23:19:16: CHEMBL659279:
2024-05-04 23:19:16: CHEMBL4012796:
2024-05-04 23:19:17: CHEMBL871030:
2024-05-04 23:19:18: CHEMBL2388192:
2024-05-04 23:19:18: CHEMBL1219014:
2024-05-04 23:19:19: CHEMBL3293254:
2024-05-04 23:19:20: CHEMBL710864:
2024-05-04 23:19:20: CHEMBL3999675:
2024-05-04 23:19:21: CHEMBL3807925:
2024-05-04 23:19:22: CHEMBL3060860:
2024-05-04 23:19:23: CHEMBL4039492:
2024-05-04 23:19:24: CHEMBL1614018:
2024-05-04 23:19:25: CHEMBL2379829:
2024-05-04 23:19:25: CHEMBL960357:
2024-05-04 23:19:25: CHEMBL3412552:
2024-05-04 23:19:26: CHEMBL914626:
2024-05-04 23:19:27: CHEMBL2154557:
2024-05-04 23:19:28: CHEMBL710381:
2024-05-04 23:19:28: CHEMBL1776383:
2024-05-04 23:19:29: CHEMBL1663702:
2024-05-04 23:19:30: CHEMBL907805:
2024-05-04 23:19:32: CHEMBL835015:
2024-05-04 23:19:34: CHEMBL3999676:
2024-05-04 23:19:35: CHEMBL924496:
2024-05-04 23:19:35: CHEMBL3062734:
2024-05-04 23:19:36: CHEMBL3123238:
2024-05-04 23:19:37: CHEMBL4721621:
2024-05-04 23:19:37: CHEMBL1033999:
2024-05-04 23:19:38: CHEMBL4276507:
2024-05-04 23:19:40: CHEMBL1012640:
2024-05-04 23:19:40: CHEMBL2410403:
2024-05-04 23:19:41: CHEMBL3292846:
2024-05-04 23:19:42: CHEMBL995359:
2024-05-04 23:19:43: CHEMBL906386:
2024-05-04 23:19:43: CHEMBL1681056:
2024-05-04 23:19:44: CHEMBL3055209:
2024-05-04 23:19:44: CHEMBL1030449:
2024-05-04 23:19:45: CHEMBL713420:
2024-05-04 23:19:46: CHEMBL1011648:
2024-05-04 23:19:46: CHEMBL3135563:
2024-05-04 23:19:47: CHEMBL676536:
2024-05-04 23:19:48: CHEMBL4008247:
2024-05-04 23:19:49: CHEMBL3776795:
2024-05-04 23:19:49: CHEMBL713934:
2024-05-04 23:19:49: CHEMBL892896:
2024-05-04 23:19:50: CHEMBL2044336:
2024-05-04 23:19:51: CHEMBL4145238:
2024-05-04 23:19:52: CHEMBL3782449:
2024-05-04 23:19:52: CHEMBL837467:
2024-05-04 23:19:53: CHEMBL902989:
2024-05-04 23:19:53: CHEMBL1664812:
2024-05-04 23:19:54: CHEMBL1033990:
2024-05-04 23:19:55: CHEMBL1646836:
2024-05-04 23:19:56: CHEMBL3811059:
2024-05-04 23:19:57: CHEMBL3243833:
2024-05-04 23:19:57: CHEMBL710372:
2024-05-04 23:19:58: CHEMBL833301:
2024-05-04 23:19:59: CHEMBL3128963:
2024-05-04 23:19:59: CHEMBL2039925:
2024-05-04 23:20:00: CHEMBL3135839:
2024-05-04 23:20:02: CHEMBL1038041:
2024-05-04 23:20:02: CHEMBL1687598:
2024-05-04 23:20:03: CHEMBL1664693:
2024-05-04 23:20:04: CHEMBL867777:
2024-05-04 23:20:05: CHEMBL3291716:
2024-05-04 23:20:06: CHEMBL3129228:
2024-05-04 23:20:06: CHEMBL2072116:
2024-05-04 23:20:07: CHEMBL710373:
2024-05-04 23:20:08: CHEMBL1219004:
2024-05-04 23:20:09: CHEMBL1259560:
2024-05-04 23:20:10: CHEMBL859285:
2024-05-04 23:20:11: CHEMBL903137:
2024-05-04 23:20:11: CHEMBL1936053:
2024-05-04 23:20:12: CHEMBL994472:
2024-05-04 23:20:13: CHEMBL3128794:
2024-05-04 23:20:14: CHEMBL2037884:
2024-05-04 23:20:14: CHEMBL3738561:
2024-05-04 23:20:16: CHEMBL1118335:
2024-05-04 23:20:16: CHEMBL906000:
2024-05-04 23:20:17: CHEMBL3271615:
2024-05-04 23:20:18: CHEMBL1654779:
2024-05-04 23:20:18: CHEMBL1259557:
2024-05-04 23:20:19: CHEMBL1051809:
2024-05-04 23:20:20: CHEMBL4134874:
2024-05-04 23:20:20: CHEMBL713935:
2024-05-04 23:20:21: CHEMBL653610:
2024-05-04 23:20:22: CHEMBL975243:
2024-05-04 23:20:22: CHEMBL859876:
2024-05-04 23:20:24: CHEMBL1034012:
2024-05-04 23:20:24: CHEMBL2400409:
2024-05-04 23:20:25: CHEMBL856486:
2024-05-04 23:20:26: CHEMBL3768642:
2024-05-04 23:20:27: CHEMBL3069079:
2024-05-04 23:20:27: CHEMBL1058083:
2024-05-04 23:20:28: CHEMBL3595591:
2024-05-04 23:20:29: CHEMBL3117566:
2024-05-04 23:20:30: CHEMBL885174:
2024-05-04 23:20:30: CHEMBL940416:
2024-05-04 23:20:30: CHEMBL3792254:
2024-05-04 23:20:31: CHEMBL711617:
2024-05-04 23:20:31: CHEMBL3389957:
2024-05-04 23:20:32: CHEMBL708489:
2024-05-04 23:20:33: CHEMBL2349727:
2024-05-04 23:20:34: CHEMBL656288:
2024-05-04 23:20:34: CHEMBL956772:
2024-05-04 23:20:35: CHEMBL1815995:
2024-05-04 23:20:36: CHEMBL1100446:
2024-05-04 23:20:37: CHEMBL1022218:
2024-05-04 23:20:38: CHEMBL989570:
2024-05-04 23:20:38: CHEMBL950928:
2024-05-04 23:20:38: CHEMBL1925635:
2024-05-04 23:20:39: CHEMBL3116998:
2024-05-04 23:20:40: CHEMBL1054757:
2024-05-04 23:20:40: CHEMBL758938:
2024-05-04 23:20:41: CHEMBL2439550:
2024-05-04 23:20:42: CHEMBL984428:
2024-05-04 23:20:43: CHEMBL1961322:
2024-05-04 23:20:44: CHEMBL1259559:
2024-05-04 23:20:44: CHEMBL1794490:
2024-05-04 23:20:45: CHEMBL895181:
2024-05-04 23:20:46: CHEMBL3368544:
2024-05-04 23:20:47: CHEMBL1794502:
2024-05-04 23:20:47: CHEMBL1817391:
2024-05-04 23:20:48: CHEMBL4039493:
2024-05-04 23:20:49: CHEMBL4002828:
2024-05-04 23:20:50: CHEMBL4002829:
2024-05-04 23:20:51: CHEMBL859126:
2024-05-04 23:20:51: CHEMBL832523:
2024-05-04 23:20:52: CHEMBL1794497:
2024-05-04 23:20:53: CHEMBL1821364:
2024-05-04 23:20:53: CHEMBL907801:
2024-05-04 23:20:54: CHEMBL3282591:
2024-05-04 23:20:55: CHEMBL3292952:
2024-05-04 23:20:56: CHEMBL709777:
2024-05-04 23:20:57: CHEMBL3381607:
2024-05-04 23:20:57: CHEMBL2400390:
2024-05-04 23:20:58: CHEMBL3122511:
2024-05-04 23:20:58: CHEMBL4012795:
2024-05-04 23:20:59: CHEMBL1108884:
2024-05-04 23:21:00: CHEMBL3375068:
2024-05-04 23:21:01: CHEMBL3778315:
2024-05-04 23:21:01: CHEMBL1217314:
2024-05-04 23:21:02: CHEMBL706841:
2024-05-04 23:21:03: CHEMBL2434102:
2024-05-04 23:21:03: CHEMBL1167859:
2024-05-04 23:21:04: CHEMBL854478:
2024-05-04 23:21:04: CHEMBL3599625:
2024-05-04 23:21:05: CHEMBL834635:
2024-05-04 23:21:06: CHEMBL1646095:
2024-05-04 23:21:07: CHEMBL4014622:
2024-05-04 23:21:08: CHEMBL1788062:
2024-05-04 23:21:08: CHEMBL833119:
2024-05-04 23:21:09: CHEMBL837207:
2024-05-04 23:21:10: CHEMBL4717401:
2024-05-04 23:21:11: CHEMBL1063960:
2024-05-04 23:21:12: CHEMBL3239483:
2024-05-04 23:21:13: CHEMBL3106982:
2024-05-04 23:21:13: CHEMBL3063083:
2024-05-04 23:21:14: CHEMBL1012639:
2024-05-04 23:21:15: CHEMBL890376:
2024-05-04 23:21:16: CHEMBL706947:
2024-05-04 23:21:16: CHEMBL653803:
2024-05-04 23:21:17: CHEMBL655544:
2024-05-04 23:21:17: CHEMBL853640:
2024-05-04 23:21:18: CHEMBL1827050:
2024-05-04 23:21:19: CHEMBL2025518:
2024-05-04 23:21:20: CHEMBL861572:
2024-05-04 23:21:21: CHEMBL3129137:
2024-05-04 23:21:22: CHEMBL3807629:
2024-05-04 23:21:23: CHEMBL708386:
2024-05-04 23:21:23: CHEMBL4276505:
2024-05-04 23:21:24: CHEMBL896487:
2024-05-04 23:21:24: CHEMBL1046026:
2024-05-04 23:21:25: CHEMBL946070:
2024-05-04 23:21:26: CHEMBL690079:
2024-05-04 23:21:27: CHEMBL2404680:
2024-05-04 23:21:27: CHEMBL3097953:
2024-05-04 23:21:28: CHEMBL865135:
2024-05-04 23:21:29: CHEMBL1953039:
2024-05-04 23:21:30: CHEMBL1648264:
2024-05-04 23:21:31: CHEMBL866685:
2024-05-04 23:21:32: CHEMBL887561:
2024-05-04 23:21:32: CHEMBL2342763:
2024-05-04 23:21:33: CHEMBL2173189:
2024-05-04 23:21:33: CHEMBL2092203:
2024-05-04 23:21:33: CHEMBL862229:
2024-05-04 23:21:34: CHEMBL656774:
2024-05-04 23:21:35: CHEMBL1053168:
2024-05-04 23:21:35: CHEMBL1033994:
2024-05-04 23:21:36: CHEMBL1794295:
2024-05-04 23:21:37: CHEMBL705091:
2024-05-04 23:21:37: CHEMBL4179594:
2024-05-04 23:21:38: CHEMBL1041705:
2024-05-04 23:21:39: CHEMBL1035510:
2024-05-04 23:21:39: CHEMBL1815047:
2024-05-04 23:21:40: CHEMBL3373177:
2024-05-04 23:21:40: CHEMBL1273622:
2024-05-04 23:21:40: CHEMBL950929:
2024-05-04 23:21:40: CHEMBL901904:
2024-05-04 23:21:41: CHEMBL3767404:
2024-05-04 23:21:42: CHEMBL4043720:
2024-05-04 23:21:42: CHEMBL864455:
2024-05-04 23:21:43: CHEMBL834743:
2024-05-04 23:21:44: CHEMBL2033223:
2024-05-04 23:21:45: CHEMBL1033991:
2024-05-04 23:21:46: CHEMBL833971:
2024-05-04 23:21:47: CHEMBL946066:
2024-05-04 23:21:47: CHEMBL1176948:
2024-05-04 23:21:48: CHEMBL907802:
2024-05-04 23:21:49: CHEMBL3057815:
2024-05-04 23:21:49: CHEMBL1776384:
2024-05-04 23:21:50: CHEMBL1035509:
2024-05-04 23:21:51: CHEMBL715738:
2024-05-04 23:21:51: CHEMBL676912:
2024-05-04 23:21:52: CHEMBL1000357:
2024-05-04 23:21:52: CHEMBL3380429:
2024-05-04 23:21:53: CHEMBL1007795:
2024-05-04 23:21:53: CHEMBL653896:
2024-05-04 23:21:54: CHEMBL3124021:
2024-05-04 23:21:55: CHEMBL1259552:
2024-05-04 23:21:56: CHEMBL2213258:
2024-05-04 23:21:56: CHEMBL3094979:
2024-05-04 23:21:57: CHEMBL3395635:
2024-05-04 23:21:58: CHEMBL1827049:
2024-05-04 23:21:58: CHEMBL1960049:
2024-05-04 23:21:59: CHEMBL3241207:
2024-05-04 23:22:00: CHEMBL955850:
2024-05-04 23:22:00: CHEMBL2015865:
2024-05-04 23:22:01: CHEMBL972377:
2024-05-04 23:22:02: CHEMBL899466:
2024-05-04 23:22:03: CHEMBL3366183:
2024-05-04 23:22:03: CHEMBL1108106:
2024-05-04 23:22:04: CHEMBL961553:
2024-05-04 23:22:04: CHEMBL921335:
2024-05-04 23:22:05: CHEMBL4269007:
2024-05-04 23:22:05: CHEMBL4010365:
2024-05-04 23:22:06: CHEMBL4432094:
2024-05-04 23:22:07: CHEMBL3583198:
2024-05-04 23:22:08: CHEMBL4197819:
2024-05-04 23:22:09: CHEMBL687226:
2024-05-04 23:22:09: CHEMBL1913092:
2024-05-04 23:22:10: CHEMBL3074007:
2024-05-04 23:22:11: CHEMBL804248:
2024-05-04 23:22:12: CHEMBL837471:
2024-05-04 23:22:12: CHEMBL827425:
2024-05-04 23:22:13: CHEMBL689615:
2024-05-04 23:22:13: CHEMBL4200978:
2024-05-04 23:22:14: CHEMBL1228543:
2024-05-04 23:22:15: CHEMBL1024562:
2024-05-04 23:22:15: CHEMBL2344963:
2024-05-04 23:22:16: CHEMBL4717404:
2024-05-04 23:22:17: CHEMBL3292845:
2024-05-04 23:22:17: CHEMBL3611931:
2024-05-04 23:22:18: CHEMBL3100980:
2024-05-04 23:22:19: CHEMBL1654781:
2024-05-04 23:22:20: CHEMBL4022472:
2024-05-04 23:22:20: CHEMBL3637832:
2024-05-04 23:22:21: CHEMBL4002826:
2024-05-04 23:22:21: CHEMBL900552:
2024-05-04 23:22:22: CHEMBL657413:
2024-05-04 23:22:23: CHEMBL3291715:
2024-05-04 23:22:24: CHEMBL2439404:
2024-05-04 23:22:25: CHEMBL904573:
2024-05-04 23:22:25: CHEMBL1115072:
2024-05-04 23:22:26: CHEMBL1259551:
2024-05-04 23:22:26: CHEMBL2406254:
2024-05-04 23:22:27: CHEMBL957277:
2024-05-04 23:22:28: CHEMBL2379640:
2024-05-04 23:22:28: CHEMBL3387003:
2024-05-04 23:22:29: CHEMBL2046228:
2024-05-04 23:22:29: CHEMBL979180:
2024-05-04 23:22:30: CHEMBL1019290:
2024-05-04 23:22:31: CHEMBL3627036:
2024-05-04 23:22:31: CHEMBL949416:
2024-05-04 23:22:31: CHEMBL2339580:
2024-05-04 23:22:32: CHEMBL714338:
2024-05-04 23:22:33: CHEMBL1960332:
2024-05-04 23:22:34: CHEMBL992597:
2024-05-04 23:22:34: CHEMBL4134181:
2024-05-04 23:22:35: CHEMBL1737867:
2024-05-04 23:22:36: CHEMBL705093:
2024-05-04 23:22:36: CHEMBL2211789:
2024-05-04 23:22:37: CHEMBL3124294:
2024-05-04 23:22:38: CHEMBL2044326:
2024-05-04 23:22:39: CHEMBL1809301:
2024-05-04 23:22:40: CHEMBL833118:
2024-05-04 23:22:41: CHEMBL1033787:
2024-05-04 23:22:41: CHEMBL728516:
2024-05-04 23:22:42: CHEMBL1635612:
2024-05-04 23:22:42: CHEMBL3295869:
2024-05-04 23:22:43: CHEMBL1685193:
2024-05-04 23:22:43: CHEMBL3794968:
2024-05-04 23:22:44: CHEMBL3744583:
2024-05-04 23:22:45: CHEMBL894309:
2024-05-04 23:22:45: CHEMBL1953145:
2024-05-04 23:22:46: CHEMBL1067568:
2024-05-04 23:22:47: CHEMBL3794901:
2024-05-04 23:22:48: CHEMBL1654783:
2024-05-04 23:22:48: CHEMBL833079:
2024-05-04 23:22:49: CHEMBL684710:
2024-05-04 23:22:49: CHEMBL3636663:
2024-05-04 23:22:50: CHEMBL710084:
2024-05-04 23:22:51: CHEMBL2351677:
2024-05-04 23:22:52: CHEMBL4220812:
2024-05-04 23:22:53: CHEMBL3227446:
2024-05-04 23:22:53: CHEMBL710531:
2024-05-04 23:22:54: CHEMBL918457:
2024-05-04 23:22:55: CHEMBL1048663:
2024-05-04 23:22:55: CHEMBL4270097:
2024-05-04 23:22:56: CHEMBL3384577:
2024-05-04 23:22:56: CHEMBL984613:
2024-05-04 23:22:57: CHEMBL1944125:
2024-05-04 23:22:58: CHEMBL1046975:
2024-05-04 23:22:58: CHEMBL960166:
2024-05-04 23:22:59: CHEMBL955136:
2024-05-04 23:23:00: CHEMBL3131181:
2024-05-04 23:23:00: CHEMBL711491:
2024-05-04 23:23:01: CHEMBL2043755:
2024-05-04 23:23:02: CHEMBL1654778:
2024-05-04 23:23:02: CHEMBL824904:
2024-05-04 23:23:03: CHEMBL980562:
2024-05-04 23:23:03: CHEMBL4011461:
2024-05-04 23:23:03: CHEMBL4028622:
2024-05-04 23:23:06: CHEMBL1109941:
2024-05-04 23:23:07: CHEMBL4263093:
2024-05-04 23:23:08: CHEMBL3047091:
2024-05-04 23:23:08: CHEMBL908409:
2024-05-04 23:23:09: CHEMBL3429681:
2024-05-04 23:23:10: CHEMBL945414:
2024-05-04 23:23:11: CHEMBL995902:
2024-05-04 23:23:11: CHEMBL1918324:
2024-05-04 23:23:11: CHEMBL918692:
2024-05-04 23:23:12: CHEMBL1960696:
2024-05-04 23:23:13: CHEMBL885171:
2024-05-04 23:23:14: CHEMBL4028610:
2024-05-04 23:23:15: CHEMBL1913093:
2024-05-04 23:23:15: CHEMBL2215267:
2024-05-04 23:23:16: CHEMBL1809762:
2024-05-04 23:23:18: CHEMBL1176167:
2024-05-04 23:23:19: CHEMBL1018953:
2024-05-04 23:23:19: CHEMBL3388462:
2024-05-04 23:23:20: CHEMBL1961051:
2024-05-04 23:23:20: CHEMBL1015953:
2024-05-04 23:23:21: CHEMBL1259558:
2024-05-04 23:23:21: CHEMBL2319498:
2024-05-04 23:23:22: CHEMBL901903:
2024-05-04 23:23:22: CHEMBL3123721:
2024-05-04 23:23:23: CHEMBL968903:
2024-05-04 23:23:24: CHEMBL656768:
2024-05-04 23:23:25: CHEMBL2339577:
2024-05-04 23:23:25: CHEMBL3782490:
2024-05-04 23:23:26: CHEMBL1936050:
2024-05-04 23:23:28: CHEMBL2059948:
2024-05-04 23:23:28: CHEMBL711490:
2024-05-04 23:23:29: CHEMBL924132:
2024-05-04 23:23:30: CHEMBL887332:
2024-05-04 23:23:31: CHEMBL1655239:
2024-05-04 23:23:32: CHEMBL712346:
2024-05-04 23:23:32: CHEMBL3773195:
2024-05-04 23:23:33: CHEMBL805604:
2024-05-04 23:23:34: CHEMBL2395313:
2024-05-04 23:23:34: CHEMBL696950:
2024-05-04 23:23:35: CHEMBL4195632:
2024-05-04 23:23:36: CHEMBL1116449:
2024-05-04 23:23:36: CHEMBL2020129:
2024-05-04 23:23:37: CHEMBL2211264:
2024-05-04 23:23:38: CHEMBL4199669:
2024-05-04 23:23:38: CHEMBL994933:
2024-05-04 23:23:39: CHEMBL3123239:
2024-05-04 23:23:40: CHEMBL2353302:
2024-05-04 23:23:41: CHEMBL4273395:
2024-05-04 23:23:41: CHEMBL2405554:
2024-05-04 23:23:42: CHEMBL894948:
2024-05-04 23:23:43: CHEMBL1646260:
2024-05-04 23:23:44: CHEMBL1769825:
2024-05-04 23:23:45: CHEMBL1012001:
2024-05-04 23:23:46: CHEMBL2015317:
2024-05-04 23:23:47: CHEMBL945413:
2024-05-04 23:23:47: CHEMBL1959200:
2024-05-04 23:23:48: CHEMBL1959201:
2024-05-04 23:23:49: CHEMBL4018204:
2024-05-04 23:23:50: CHEMBL1655237:
2024-05-04 23:23:50: CHEMBL2320857:
2024-05-04 23:23:51: CHEMBL854523:
2024-05-04 23:23:52: CHEMBL4011459:
2024-05-04 23:23:52: CHEMBL4010364:
2024-05-04 23:23:53: CHEMBL2422883:
2024-05-04 23:23:53: CHEMBL903138:
2024-05-04 23:23:54: CHEMBL1071243:
2024-05-04 23:23:54: CHEMBL3784851:
2024-05-04 23:23:55: CHEMBL980716:
2024-05-04 23:23:56: CHEMBL864208:
2024-05-04 23:23:57: CHEMBL866752:
2024-05-04 23:23:57: CHEMBL867244:
2024-05-04 23:23:58: CHEMBL685389:
2024-05-04 23:23:59: CHEMBL4479167:
2024-05-04 23:23:59: CHEMBL4276508:
2024-05-04 23:24:00: CHEMBL833214:
2024-05-04 23:24:00: CHEMBL3795205:
2024-05-04 23:24:01: CHEMBL2439553:
2024-05-04 23:24:02: CHEMBL1803361:
2024-05-04 23:24:03: CHEMBL949071:
2024-05-04 23:24:03: CHEMBL3778340:
2024-05-04 23:24:04: CHEMBL684103:
2024-05-04 23:24:05: CHEMBL659221:
2024-05-04 23:24:05: CHEMBL3122917:
2024-05-04 23:24:06: CHEMBL934248:
2024-05-04 23:24:07: CHEMBL897796:
2024-05-04 23:24:07: CHEMBL3812459:
2024-05-04 23:24:07: CHEMBL907804:
2024-05-04 23:24:08: CHEMBL877209:
2024-05-04 23:24:09: CHEMBL1228470:
2024-05-04 23:24:10: CHEMBL968905:
Total norm: 6.775315984160002
Params with grad: 88762
Params without grad: 20
Current learning rate: [1.0, 0.001]
Total norm: 11.97557645925549
Params with grad: 88762
Params without grad: 20
Current learning rate: [1.0, 0.002]
Total norm: 16.402408578565847
Params with grad: 88762
Params without grad: 20
Current learning rate: [1.0, 0.003]
Total norm: 19.247279707652982
Params with grad: 88762
Params without grad: 20
Current learning rate: [1.0, 0.004]
Total norm: 21.276596025230337
Params with grad: 88762
Params without grad: 20
Current learning rate: [1.0, 0.005000000000000001]
Total norm: 24.54225434091814
Params with grad: 88762
Params without grad: 20
Current learning rate: [1.0, 0.006]
Total norm: 29.47116648291097
Params with grad: 88762
Params without grad: 20
Current learning rate: [1.0, 0.007000000000000001]
Total norm: 32.3035139119972
Params with grad: 88762
Params without grad: 20
Current learning rate: [1.0, 0.008]
Total norm: 36.058178502471385
Params with grad: 88762
Params without grad: 20
Current learning rate: [1.0, 0.009]
2024-05-04 23:25:46: CHEMBL1913092:
2024-05-04 23:25:46: CHEMBL995355:
2024-05-04 23:25:46: CHEMBL687226:
2024-05-04 23:25:46: CHEMBL3387003:
2024-05-04 23:25:46: CHEMBL3389542:
2024-05-04 23:25:46: CHEMBL833080:
2024-05-04 23:25:46: CHEMBL972377:
2024-05-04 23:25:46: CHEMBL4010365:
2024-05-04 23:25:46: CHEMBL3380429:
2024-05-04 23:25:46: CHEMBL713934:
2024-05-04 23:25:46: CHEMBL804251:
2024-05-04 23:25:46: CHEMBL1052565:
2024-05-04 23:25:46: CHEMBL3418701:
2024-05-04 23:25:46: CHEMBL3413997:
2024-05-04 23:25:46: CHEMBL710531:
2024-05-04 23:25:46: CHEMBL689615:
2024-05-04 23:25:46: CHEMBL902229:
2024-05-04 23:25:46: CHEMBL4253251:
2024-05-04 23:25:46: CHEMBL1219014:
2024-05-04 23:25:46: CHEMBL1011648:
2024-05-04 23:25:46: CHEMBL1022218:
2024-05-04 23:25:46: CHEMBL837207:
2024-05-04 23:25:46: CHEMBL2349727:
2024-05-04 23:25:46: CHEMBL685389:
2024-05-04 23:25:46: CHEMBL3744583:
2024-05-04 23:25:46: CHEMBL833378:
2024-05-04 23:25:46: CHEMBL1033999:
2024-05-04 23:25:46: CHEMBL833119:
2024-05-04 23:25:46: CHEMBL3807925:
2024-05-04 23:25:46: CHEMBL1259550:
2024-05-04 23:25:46: CHEMBL1776384:
2024-05-04 23:25:46: CHEMBL1167859:
2024-05-04 23:25:46: CHEMBL2395313:
2024-05-04 23:25:46: CHEMBL710534:
2024-05-04 23:25:46: CHEMBL2339580:
2024-05-04 23:25:46: CHEMBL3367829:
2024-05-04 23:25:46: CHEMBL907802:
2024-05-04 23:25:46: CHEMBL3074007:
2024-05-04 23:25:46: CHEMBL887332:
2024-05-04 23:25:46: CHEMBL1053168:
2024-05-04 23:25:46: CHEMBL3794968:
2024-05-04 23:25:46: CHEMBL1035509:
2024-05-04 23:25:46: CHEMBL980562:
2024-05-04 23:25:46: CHEMBL992597:
2024-05-04 23:25:46: CHEMBL2351677:
2024-05-04 23:25:46: CHEMBL918634:
2024-05-04 23:25:46: CHEMBL656768:
2024-05-04 23:25:46: CHEMBL2439404:
2024-05-04 23:25:46: CHEMBL4195632:
2024-05-04 23:25:46: CHEMBL946070:
2024-05-04 23:25:46: CHEMBL713935:
2024-05-04 23:25:46: CHEMBL907801:
2024-05-04 23:25:46: CHEMBL833214:
2024-05-04 23:25:46: CHEMBL3776795:
2024-05-04 23:25:46: CHEMBL1041705:
2024-05-04 23:25:46: CHEMBL3069079:
2024-05-04 23:25:46: CHEMBL703466:
2024-05-04 23:25:46: CHEMBL897415:
2024-05-04 23:25:46: CHEMBL833971:
2024-05-04 23:25:46: CHEMBL1058083:
2024-05-04 23:25:46: CHEMBL710381:
2024-05-04 23:25:46: CHEMBL3063083:
2024-05-04 23:25:46: CHEMBL4197926:
2024-05-04 23:25:47: CHEMBL3078730:
2024-05-04 23:25:47: CHEMBL3390310:
2024-05-04 23:25:47: CHEMBL867777:
2024-05-04 23:25:47: CHEMBL4136409:
2024-05-04 23:25:47: CHEMBL4002829:
2024-05-04 23:25:47: CHEMBL4028610:
2024-05-04 23:25:47: CHEMBL1794502:
2024-05-04 23:25:47: CHEMBL1008279:
2024-05-04 23:25:47: CHEMBL1121169:
2024-05-04 23:25:47: CHEMBL2388705:
2024-05-04 23:25:47: CHEMBL3055209:
2024-05-04 23:25:47: CHEMBL3429681:
2024-05-04 23:25:47: CHEMBL655544:
2024-05-04 23:25:47: CHEMBL4012795:
2024-05-04 23:25:47: CHEMBL712346:
2024-05-04 23:25:47: CHEMBL3784851:
2024-05-04 23:25:47: CHEMBL713255:
2024-05-04 23:25:47: CHEMBL3073615:
2024-05-04 23:25:47: CHEMBL697276:
2024-05-04 23:25:47: CHEMBL837467:
2024-05-04 23:25:47: CHEMBL833118:
2024-05-04 23:25:47: CHEMBL1816523:
2024-05-04 23:25:47: CHEMBL1932094:
2024-05-04 23:25:47: CHEMBL833855:
2024-05-04 23:25:47: CHEMBL1108884:
2024-05-04 23:25:47: CHEMBL3792254:
2024-05-04 23:25:47: CHEMBL860972:
2024-05-04 23:25:47: CHEMBL859876:
2024-05-04 23:25:47: CHEMBL4011459:
2024-05-04 23:25:47: CHEMBL946064:
2024-05-04 23:25:47: CHEMBL1107997:
2024-05-04 23:25:47: CHEMBL3227446:
2024-05-04 23:25:47: CHEMBL903139:
2024-05-04 23:25:47: CHEMBL885174:
2024-05-04 23:25:47: CHEMBL1176167:
2024-05-04 23:25:47: CHEMBL2150628:
2024-05-04 23:25:47: CHEMBL3122349:
2024-05-04 23:25:47: CHEMBL4270097:
2024-05-04 23:25:48: CHEMBL1635612:
2024-05-04 23:25:48: CHEMBL861572:
2024-05-04 23:25:48: CHEMBL1827050:
2024-05-04 23:25:48: CHEMBL863014:
2024-05-04 23:25:48: CHEMBL945414:
2024-05-04 23:25:48: CHEMBL1953145:
2024-05-04 23:25:48: CHEMBL2432161:
2024-05-04 23:25:48: CHEMBL4717404:
2024-05-04 23:25:48: CHEMBL1051809:
2024-05-04 23:25:48: CHEMBL1019290:
2024-05-04 23:25:48: CHEMBL728516:
2024-05-04 23:25:48: CHEMBL865135:
2024-05-04 23:25:48: CHEMBL3123239:
2024-05-04 23:25:48: CHEMBL2154555:
2024-05-04 23:25:48: CHEMBL1960332:
2024-05-04 23:25:48: CHEMBL1109941:
2024-05-04 23:25:48: CHEMBL937510:
2024-05-04 23:25:48: CHEMBL3815782:
2024-05-04 23:25:48: CHEMBL1035506:
2024-05-04 23:25:48: CHEMBL1045728:
2024-05-04 23:25:48: CHEMBL1769825:
2024-05-04 23:25:48: CHEMBL944781:
2024-05-04 23:25:48: CHEMBL2210037:
2024-05-04 23:25:48: CHEMBL862229:
2024-05-04 23:25:48: CHEMBL2059948:
2024-05-04 23:25:48: CHEMBL2173189:
2024-05-04 23:25:48: CHEMBL1024879:
2024-05-04 23:25:48: CHEMBL921335:
2024-05-04 23:25:48: CHEMBL4008247:
2024-05-04 23:25:48: CHEMBL1614018:
2024-05-04 23:25:51: CHEMBL956847:
2024-05-04 23:25:52: CHEMBL1259552:
2024-05-04 23:25:53: CHEMBL3807629:
2024-05-04 23:25:53: CHEMBL867244:
2024-05-04 23:25:54: CHEMBL4012796:
2024-05-04 23:25:55: CHEMBL853640:
2024-05-04 23:25:55: CHEMBL834635:
2024-05-04 23:25:56: CHEMBL2390255:
2024-05-04 23:25:56: CHEMBL2342597:
2024-05-04 23:25:57: CHEMBL1053306:
2024-05-04 23:25:58: CHEMBL4479167:
2024-05-04 23:25:59: CHEMBL922937:
2024-05-04 23:26:00: CHEMBL3295663:
2024-05-04 23:26:00: CHEMBL3124021:
2024-05-04 23:26:00: CHEMBL1259558:
2024-05-04 23:26:01: CHEMBL1048663:
2024-05-04 23:26:02: CHEMBL1654778:
2024-05-04 23:26:02: CHEMBL3381607:
2024-05-04 23:26:03: CHEMBL657413:
2024-05-04 23:26:04: CHEMBL4050758:
2024-05-04 23:26:05: CHEMBL833076:
2024-05-04 23:26:06: CHEMBL3094979:
2024-05-04 23:26:06: CHEMBL2390645:
2024-05-04 23:26:07: CHEMBL1921081:
2024-05-04 23:26:08: CHEMBL922474:
2024-05-04 23:26:08: CHEMBL3418917:
2024-05-04 23:26:09: CHEMBL4022472:
2024-05-04 23:26:09: CHEMBL4134874:
2024-05-04 23:26:10: CHEMBL2405555:
2024-05-04 23:26:10: CHEMBL690079:
2024-05-04 23:26:11: CHEMBL1794398:
2024-05-04 23:26:12: CHEMBL1046026:
2024-05-04 23:26:13: CHEMBL1071243:
2024-05-04 23:26:13: CHEMBL3129137:
2024-05-04 23:26:14: CHEMBL3627036:
2024-05-04 23:26:14: CHEMBL835015:
2024-05-04 23:26:16: CHEMBL1018953:
2024-05-04 23:26:16: CHEMBL940416:
2024-05-04 23:26:16: CHEMBL3592342:
2024-05-04 23:26:17: CHEMBL1219004:
2024-05-04 23:26:18: CHEMBL1024326:
2024-05-04 23:26:18: CHEMBL1918205:
2024-05-04 23:26:19: CHEMBL3812459:
2024-05-04 23:26:19: CHEMBL1073156:
2024-05-04 23:26:19: CHEMBL1015953:
2024-05-04 23:26:20: CHEMBL2213258:
2024-05-04 23:26:21: CHEMBL1681056:
2024-05-04 23:26:22: CHEMBL659279:
2024-05-04 23:26:22: CHEMBL1046975:
2024-05-04 23:26:23: CHEMBL2043755:
2024-05-04 23:26:24: CHEMBL4008248:
2024-05-04 23:26:25: CHEMBL3295321:
2024-05-04 23:26:25: CHEMBL1024562:
2024-05-04 23:26:26: CHEMBL1664693:
2024-05-04 23:26:27: CHEMBL3123043:
2024-05-04 23:26:28: CHEMBL3129215:
2024-05-04 23:26:28: CHEMBL864455:
2024-05-04 23:26:29: CHEMBL866752:
2024-05-04 23:26:29: CHEMBL907807:
2024-05-04 23:26:30: CHEMBL3077285:
2024-05-04 23:26:31: CHEMBL1058849:
2024-05-04 23:26:32: CHEMBL1655239:
2024-05-04 23:26:32: CHEMBL2046228:
2024-05-04 23:26:33: CHEMBL2051598:
2024-05-04 23:26:34: CHEMBL1815995:
2024-05-04 23:26:35: CHEMBL1816524:
2024-05-04 23:26:35: CHEMBL3241207:
2024-05-04 23:26:36: CHEMBL1020690:
2024-05-04 23:26:37: CHEMBL1035508:
2024-05-04 23:26:37: CHEMBL4145238:
2024-05-04 23:26:38: CHEMBL2410403:
2024-05-04 23:26:39: CHEMBL2434102:
2024-05-04 23:26:39: CHEMBL3412552:
2024-05-04 23:26:40: CHEMBL1109063:
2024-05-04 23:26:40: CHEMBL4269007:
2024-05-04 23:26:41: CHEMBL1034012:
2024-05-04 23:26:42: CHEMBL1664812:
2024-05-04 23:26:42: CHEMBL689145:
2024-05-04 23:26:44: CHEMBL1803360:
2024-05-04 23:26:44: CHEMBL711491:
2024-05-04 23:26:45: CHEMBL708445:
2024-05-04 23:26:46: CHEMBL4263092:
2024-05-04 23:26:46: CHEMBL1837558:
2024-05-04 23:26:47: CHEMBL1108106:
2024-05-04 23:26:47: CHEMBL4220813:
2024-05-04 23:26:48: CHEMBL4717401:
2024-05-04 23:26:49: CHEMBL656288:
2024-05-04 23:26:50: CHEMBL1788062:
2024-05-04 23:26:51: CHEMBL994933:
2024-05-04 23:26:52: CHEMBL1944125:
2024-05-04 23:26:52: CHEMBL1067060:
2024-05-04 23:26:53: CHEMBL907805:
2024-05-04 23:26:54: CHEMBL854523:
2024-05-04 23:26:55: CHEMBL4200978:
2024-05-04 23:26:55: CHEMBL1663615:
2024-05-04 23:26:56: CHEMBL1273622:
2024-05-04 23:26:56: CHEMBL710084:
2024-05-04 23:26:57: CHEMBL659221:
2024-05-04 23:26:57: CHEMBL2015865:
2024-05-04 23:26:58: CHEMBL3395635:
2024-05-04 23:26:59: CHEMBL814078:
2024-05-04 23:27:00: CHEMBL3122629:
2024-05-04 23:27:00: CHEMBL4187321:
2024-05-04 23:27:01: CHEMBL759088:
2024-05-04 23:27:02: CHEMBL4432407:
2024-05-04 23:27:03: CHEMBL712878:
2024-05-04 23:27:03: CHEMBL3782449:
2024-05-04 23:27:04: CHEMBL960357:
2024-05-04 23:27:05: CHEMBL4273395:
2024-05-04 23:27:06: CHEMBL4002826:
2024-05-04 23:27:07: CHEMBL3366183:
2024-05-04 23:27:08: CHEMBL3368544:
2024-05-04 23:27:09: CHEMBL1953907:
2024-05-04 23:27:09: CHEMBL877209:
2024-05-04 23:27:10: CHEMBL827425:
2024-05-04 23:27:11: CHEMBL1794295:
2024-05-04 23:27:12: CHEMBL903138:
2024-05-04 23:27:12: CHEMBL2353302:
2024-05-04 23:27:13: CHEMBL3295870:
2024-05-04 23:27:14: CHEMBL856486:
2024-05-04 23:27:14: CHEMBL706947:
2024-05-04 23:27:15: CHEMBL897796:
2024-05-04 23:27:16: CHEMBL924132:
2024-05-04 23:27:17: CHEMBL4006616:
2024-05-04 23:27:17: CHEMBL1936129:
2024-05-04 23:27:18: CHEMBL968903:
2024-05-04 23:27:19: CHEMBL1063960:
2024-05-04 23:27:19: CHEMBL3578893:
2024-05-04 23:27:20: CHEMBL3588714:
2024-05-04 23:27:20: CHEMBL1961322:
2024-05-04 23:27:21: CHEMBL3282588:
2024-05-04 23:27:22: CHEMBL1033994:
2024-05-04 23:27:23: CHEMBL980716:
2024-05-04 23:27:23: CHEMBL984428:
2024-05-04 23:27:24: CHEMBL1809301:
2024-05-04 23:27:25: CHEMBL1034018:
2024-05-04 23:27:25: CHEMBL2404680:
2024-05-04 23:27:26: CHEMBL4014622:
2024-05-04 23:27:27: CHEMBL653610:
2024-05-04 23:27:28: CHEMBL1832560:
2024-05-04 23:27:29: CHEMBL866685:
2024-05-04 23:27:29: CHEMBL1737867:
2024-05-04 23:27:30: CHEMBL1176948:
2024-05-04 23:27:30: CHEMBL2406254:
2024-05-04 23:27:30: CHEMBL4276508:
2024-05-04 23:27:31: CHEMBL2405554:
2024-05-04 23:27:32: CHEMBL1767670:
2024-05-04 23:27:32: CHEMBL4264727:
2024-05-04 23:27:33: CHEMBL824904:
2024-05-04 23:27:34: CHEMBL3384578:
2024-05-04 23:27:34: CHEMBL1116449:
2024-05-04 23:27:35: CHEMBL4194391:
2024-05-04 23:27:36: CHEMBL1646095:
2024-05-04 23:27:37: CHEMBL1936053:
2024-05-04 23:27:37: CHEMBL3595591:
2024-05-04 23:27:38: CHEMBL708386:
2024-05-04 23:27:39: CHEMBL1803361:
2024-05-04 23:27:39: CHEMBL3761618:
2024-05-04 23:27:40: CHEMBL1051810:
2024-05-04 23:27:40: CHEMBL1654781:
2024-05-04 23:27:41: CHEMBL901903:
2024-05-04 23:27:42: CHEMBL901904:
2024-05-04 23:27:43: CHEMBL3773195:
2024-05-04 23:27:43: CHEMBL936680:
2024-05-04 23:27:44: CHEMBL4719106:
2024-05-04 23:27:45: CHEMBL4276505:
2024-05-04 23:27:45: CHEMBL859126:
2024-05-04 23:27:46: CHEMBL3389957:
2024-05-04 23:27:47: CHEMBL4054609:
2024-05-04 23:27:48: CHEMBL1654783:
2024-05-04 23:27:49: CHEMBL676912:
2024-05-04 23:27:49: CHEMBL3767404:
2024-05-04 23:27:50: CHEMBL906490:
2024-05-04 23:27:51: CHEMBL945413:
2024-05-04 23:27:51: CHEMBL1228542:
2024-05-04 23:27:53: CHEMBL2388192:
2024-05-04 23:27:54: CHEMBL834743:
2024-05-04 23:27:54: CHEMBL3106982:
2024-05-04 23:27:55: CHEMBL1817391:
2024-05-04 23:27:56: CHEMBL684103:
2024-05-04 23:27:56: CHEMBL1033991:
2024-05-04 23:27:57: CHEMBL3122917:
2024-05-04 23:27:58: CHEMBL1960049:
2024-05-04 23:27:58: CHEMBL1260089:
2024-05-04 23:27:59: CHEMBL1918204:
2024-05-04 23:28:00: CHEMBL960166:
2024-05-04 23:28:01: CHEMBL2319498:
2024-05-04 23:28:02: CHEMBL714338:
2024-05-04 23:28:02: CHEMBL1918324:
2024-05-04 23:28:02: CHEMBL949416:
2024-05-04 23:28:03: CHEMBL860970:
2024-05-04 23:28:04: CHEMBL871033:
2024-05-04 23:28:04: CHEMBL1648264:
2024-05-04 23:28:05: CHEMBL1960696:
2024-05-04 23:28:06: CHEMBL758938:
2024-05-04 23:28:07: CHEMBL3240111:
2024-05-04 23:28:08: CHEMBL955850:
2024-05-04 23:28:08: CHEMBL885171:
2024-05-04 23:28:09: CHEMBL1067568:
2024-05-04 23:28:10: CHEMBL1100446:
2024-05-04 23:28:10: CHEMBL1925635:
2024-05-04 23:28:11: CHEMBL2162371:
2024-05-04 23:28:12: CHEMBL1815047:
2024-05-04 23:28:12: CHEMBL3994798:
2024-05-04 23:28:13: CHEMBL2211264:
2024-05-04 23:28:14: CHEMBL2445761:
2024-05-04 23:28:15: CHEMBL902989:
2024-05-04 23:28:16: CHEMBL887635:
2024-05-04 23:28:16: CHEMBL956772:
2024-05-04 23:28:17: CHEMBL4721621:
2024-05-04 23:28:18: CHEMBL924692:
2024-05-04 23:28:22: CHEMBL2020129:
2024-05-04 23:28:23: CHEMBL1646260:
2024-05-04 23:28:23: CHEMBL3097953:
2024-05-04 23:28:24: CHEMBL949071:
2024-05-04 23:28:25: CHEMBL889768:
2024-05-04 23:28:26: CHEMBL2046046:
2024-05-04 23:28:28: CHEMBL1794497:
2024-05-04 23:28:28: CHEMBL1936050:
2024-05-04 23:28:29: CHEMBL1054762:
2024-05-04 23:28:30: CHEMBL984606:
2024-05-04 23:28:31: CHEMBL653896:
2024-05-04 23:28:32: CHEMBL3057815:
2024-05-04 23:28:33: CHEMBL1012001:
2024-05-04 23:28:34: CHEMBL3239483:
2024-05-04 23:28:34: CHEMBL1059735:
2024-05-04 23:28:35: CHEMBL904573:
2024-05-04 23:28:36: CHEMBL683837:
2024-05-04 23:28:37: CHEMBL4039493:
2024-05-04 23:28:38: CHEMBL956777:
2024-05-04 23:28:38: CHEMBL758925:
2024-05-04 23:28:39: CHEMBL1033990:
2024-05-04 23:28:40: CHEMBL4018204:
2024-05-04 23:28:40: CHEMBL3243833:
2024-05-04 23:28:41: CHEMBL3243451:
2024-05-04 23:28:42: CHEMBL3129213:
2024-05-04 23:28:42: CHEMBL1817498:
2024-05-04 23:28:43: CHEMBL1116600:
2024-05-04 23:28:44: CHEMBL3060860:
2024-05-04 23:28:45: CHEMBL934248:
2024-05-04 23:28:45: CHEMBL805604:
2024-05-04 23:28:46: CHEMBL2215267:
2024-05-04 23:28:47: CHEMBL4019375:
2024-05-04 23:28:48: CHEMBL1115072:
2024-05-04 23:28:49: CHEMBL676536:
2024-05-04 23:28:49: CHEMBL918457:
2024-05-04 23:28:50: CHEMBL4002824:
2024-05-04 23:28:51: CHEMBL711490:
2024-05-04 23:28:52: CHEMBL2072118:
2024-05-04 23:28:53: CHEMBL4011461:
2024-05-04 23:28:54: CHEMBL2037884:
2024-05-04 23:28:55: CHEMBL713254:
2024-05-04 23:28:55: CHEMBL1068293:
2024-05-04 23:28:56: CHEMBL2341227:
2024-05-04 23:28:56: CHEMBL1959201:
2024-05-04 23:28:57: CHEMBL1107214:
2024-05-04 23:28:58: CHEMBL1054757:
2024-05-04 23:28:58: CHEMBL4194448:
2024-05-04 23:28:59: CHEMBL1017067:
2024-05-04 23:29:00: CHEMBL3129228:
2024-05-04 23:29:03: CHEMBL907804:
2024-05-04 23:29:04: CHEMBL2317095:
2024-05-04 23:29:04: CHEMBL3418331:
2024-05-04 23:29:05: CHEMBL711656:
2024-05-04 23:29:06: CHEMBL908409:
2024-05-04 23:29:06: CHEMBL1035510:
2024-05-04 23:29:07: CHEMBL2167912:
2024-05-04 23:29:08: CHEMBL984423:
2024-05-04 23:29:09: CHEMBL2092203:
2024-05-04 23:29:09: CHEMBL3611931:
2024-05-04 23:29:10: CHEMBL715738:
2024-05-04 23:29:11: CHEMBL1821364:
2024-05-04 23:29:11: CHEMBL906000:
2024-05-04 23:29:12: CHEMBL896487:
2024-05-04 23:29:13: CHEMBL1654782:
2024-05-04 23:29:13: CHEMBL1043900:
2024-05-04 23:29:14: CHEMBL895181:
2024-05-04 23:29:15: CHEMBL2344963:
2024-05-04 23:29:15: CHEMBL1930242:
2024-05-04 23:29:16: CHEMBL924496:
2024-05-04 23:29:17: CHEMBL3124294:
2024-05-04 23:29:18: CHEMBL3636663:
2024-05-04 23:29:18: CHEMBL894948:
2024-05-04 23:29:19: CHEMBL695986:
2024-05-04 23:29:20: CHEMBL3295662:
2024-05-04 23:29:20: CHEMBL4184680:
2024-05-04 23:29:21: CHEMBL691343:
2024-05-04 23:29:21: CHEMBL833079:
2024-05-04 23:29:22: CHEMBL1259551:
2024-05-04 23:29:22: CHEMBL1794318:
2024-05-04 23:29:23: CHEMBL3375068:
2024-05-04 23:29:24: CHEMBL1959200:
2024-05-04 23:29:25: CHEMBL2320857:
2024-05-04 23:29:26: CHEMBL837471:
2024-05-04 23:29:27: CHEMBL4179593:
2024-05-04 23:29:27: CHEMBL3291716:
2024-05-04 23:29:28: CHEMBL863382:
2024-05-04 23:29:28: CHEMBL895182:
2024-05-04 23:29:29: CHEMBL833117:
2024-05-04 23:29:30: CHEMBL3047091:
2024-05-04 23:29:31: CHEMBL1047854:
2024-05-04 23:29:32: CHEMBL2051603:
2024-05-04 23:29:32: CHEMBL950928:
2024-05-04 23:29:33: CHEMBL1055019:
2024-05-04 23:29:34: CHEMBL957277:
2024-05-04 23:29:34: CHEMBL1039164:
2024-05-04 23:29:35: CHEMBL1809762:
2024-05-04 23:29:36: CHEMBL1259557:
2024-05-04 23:29:37: CHEMBL1037279:
2024-05-04 23:29:37: CHEMBL684710:
2024-05-04 23:29:37: CHEMBL3637832:
2024-05-04 23:29:39: CHEMBL864208:
2024-05-04 23:29:39: CHEMBL1006568:
2024-05-04 23:29:40: CHEMBL4010364:
2024-05-04 23:29:41: CHEMBL994472:
2024-05-04 23:29:41: CHEMBL890376:
2024-05-04 23:29:42: CHEMBL2379640:
2024-05-04 23:29:43: CHEMBL889770:
2024-05-04 23:29:44: CHEMBL3373177:
2024-05-04 23:29:44: CHEMBL710373:
2024-05-04 23:29:45: CHEMBL4199669:
2024-05-04 23:29:45: CHEMBL1033787:
2024-05-04 23:29:46: CHEMBL995902:
2024-05-04 23:29:47: CHEMBL832523:
2024-05-04 23:29:47: CHEMBL713409:
2024-05-04 23:29:48: CHEMBL705093:
2024-05-04 23:29:48: CHEMBL2033222:
2024-05-04 23:29:49: CHEMBL1654780:
2024-05-04 23:29:50: CHEMBL3292952:
2024-05-04 23:29:51: CHEMBL906386:
2024-05-04 23:29:51: CHEMBL2019245:
2024-05-04 23:29:52: CHEMBL3405593:
2024-05-04 23:29:53: CHEMBL3123238:
2024-05-04 23:29:54: CHEMBL1012639:
2024-05-04 23:29:55: CHEMBL873604:
2024-05-04 23:29:56: CHEMBL1646836:
2024-05-04 23:29:57: CHEMBL984427:
2024-05-04 23:29:58: CHEMBL1056891:
2024-05-04 23:29:58: CHEMBL706945:
2024-05-04 23:29:59: CHEMBL984613:
2024-05-04 23:30:00: CHEMBL933048:
2024-05-04 23:30:00: CHEMBL2033223:
2024-05-04 23:30:01: CHEMBL2024927:
2024-05-04 23:30:02: CHEMBL975243:
2024-05-04 23:30:02: CHEMBL696950:
2024-05-04 23:30:03: CHEMBL3999676:
2024-05-04 23:30:04: CHEMBL804248:
2024-05-04 23:30:06: CHEMBL955136:
2024-05-04 23:30:07: CHEMBL1030449:
2024-05-04 23:30:08: CHEMBL1941610:
2024-05-04 23:30:09: CHEMBL2183320:
2024-05-04 23:30:10: CHEMBL968900:
2024-05-04 23:30:11: CHEMBL3122511:
2024-05-04 23:30:12: CHEMBL3291715:
2024-05-04 23:30:12: CHEMBL899618:
2024-05-04 23:30:12: CHEMBL2044326:
2024-05-04 23:30:13: CHEMBL3131181:
2024-05-04 23:30:14: CHEMBL1913093:
2024-05-04 23:30:14: CHEMBL2211789:
2024-05-04 23:30:15: CHEMBL833301:
2024-05-04 23:30:16: CHEMBL1007795:
2024-05-04 23:30:16: CHEMBL3285745:
2024-05-04 23:30:17: CHEMBL1664688:
2024-05-04 23:30:17: CHEMBL2067378:
2024-05-04 23:30:18: CHEMBL4039492:
2024-05-04 23:30:18: CHEMBL3271615:
2024-05-04 23:30:20: CHEMBL705091:
2024-05-04 23:30:20: CHEMBL894309:
2024-05-04 23:30:21: CHEMBL1177528:
2024-05-04 23:30:22: CHEMBL914626:
2024-05-04 23:30:22: CHEMBL833990:
2024-05-04 23:30:22: CHEMBL3795205:
2024-05-04 23:30:23: CHEMBL833275:
2024-05-04 23:30:23: CHEMBL1018957:
2024-05-04 23:30:24: CHEMBL4276506:
2024-05-04 23:30:25: CHEMBL1012640:
2024-05-04 23:30:25: CHEMBL3383460:
2024-05-04 23:30:26: CHEMBL1038041:
2024-05-04 23:30:27: CHEMBL934825:
2024-05-04 23:30:27: CHEMBL961554:
2024-05-04 23:30:28: CHEMBL900552:
2024-05-04 23:30:29: CHEMBL3384577:
2024-05-04 23:30:30: CHEMBL1259559:
2024-05-04 23:30:30: CHEMBL1228470:
2024-05-04 23:30:31: CHEMBL903137:
2024-05-04 23:30:32: CHEMBL1953039:
2024-05-04 23:30:33: CHEMBL859285:
2024-05-04 23:30:34: CHEMBL3794901:
2024-05-04 23:30:34: CHEMBL4028622:
2024-05-04 23:30:35: CHEMBL3293254:
2024-05-04 23:30:36: CHEMBL833931:
2024-05-04 23:30:36: CHEMBL944749:
2024-05-04 23:30:37: CHEMBL1228543:
2024-05-04 23:30:38: CHEMBL3292846:
2024-05-04 23:30:38: CHEMBL4263093:
2024-05-04 23:30:39: CHEMBL4179594:
2024-05-04 23:30:40: CHEMBL892896:
2024-05-04 23:30:41: CHEMBL887561:
2024-05-04 23:30:41: CHEMBL3295320:
2024-05-04 23:30:42: CHEMBL4002825:
2024-05-04 23:30:42: CHEMBL1654779:
2024-05-04 23:30:43: CHEMBL2044336:
2024-05-04 23:30:44: CHEMBL1794490:
2024-05-04 23:30:44: CHEMBL3388462:
2024-05-04 23:30:45: CHEMBL710372:
2024-05-04 23:30:46: CHEMBL3122259:
2024-05-04 23:30:46: CHEMBL3116998:
2024-05-04 23:30:47: CHEMBL1827049:
2024-05-04 23:30:47: CHEMBL871030:
2024-05-04 23:30:48: CHEMBL4432094:
2024-05-04 23:30:49: CHEMBL3096264:
2024-05-04 23:30:49: CHEMBL950929:
2024-05-04 23:30:49: CHEMBL1034017:
2024-05-04 23:30:50: CHEMBL2025518:
2024-05-04 23:30:51: CHEMBL3100980:
2024-05-04 23:30:52: CHEMBL2405783:
2024-05-04 23:30:52: CHEMBL1776383:
2024-05-04 23:30:53: CHEMBL2400409:
2024-05-04 23:30:54: CHEMBL713420:
2024-05-04 23:30:55: CHEMBL1053167:
2024-05-04 23:30:55: CHEMBL711617:
2024-05-04 23:30:56: CHEMBL4134181:
2024-05-04 23:30:56: CHEMBL710864:
2024-05-04 23:30:57: CHEMBL2447603:
2024-05-04 23:30:57: CHEMBL656397:
2024-05-04 23:30:58: CHEMBL1817497:
2024-05-04 23:30:59: CHEMBL761987:
2024-05-04 23:30:59: CHEMBL653803:
2024-05-04 23:31:00: CHEMBL854478:
2024-05-04 23:31:01: CHEMBL3088595:
2024-05-04 23:31:01: CHEMBL4002828:
2024-05-04 23:31:02: CHEMBL3135563:
2024-05-04 23:31:03: CHEMBL833792:
2024-05-04 23:31:03: CHEMBL3295869:
2024-05-04 23:31:04: CHEMBL2154557:
2024-05-04 23:31:05: CHEMBL1000357:
2024-05-04 23:31:06: CHEMBL3782490:
2024-05-04 23:31:07: CHEMBL3117566:
2024-05-04 23:31:08: CHEMBL3778315:
2024-05-04 23:31:09: CHEMBL1259560:
2024-05-04 23:31:09: CHEMBL3128794:
2024-05-04 23:31:10: CHEMBL706841:
2024-05-04 23:31:10: CHEMBL2379829:
2024-05-04 23:31:11: CHEMBL3227331:
2024-05-04 23:31:11: CHEMBL1685193:
2024-05-04 23:31:12: CHEMBL3396060:
2024-05-04 23:31:13: CHEMBL3599625:
2024-05-04 23:31:14: CHEMBL3266569:
2024-05-04 23:31:14: CHEMBL2379584:
2024-05-04 23:31:15: CHEMBL3778340:
2024-05-04 23:31:16: CHEMBL946066:
2024-05-04 23:31:16: CHEMBL1815005:
2024-05-04 23:31:17: CHEMBL2422883:
2024-05-04 23:31:17: CHEMBL3128963:
2024-05-04 23:31:18: CHEMBL995359:
2024-05-04 23:31:19: CHEMBL3738561:
2024-05-04 23:31:20: CHEMBL1961051:
2024-05-04 23:31:20: CHEMBL3062734:
2024-05-04 23:31:20: CHEMBL1033996:
2024-05-04 23:31:21: CHEMBL1217314:
2024-05-04 23:31:21: CHEMBL2339577:
2024-05-04 23:31:22: CHEMBL698789:
2024-05-04 23:31:22: CHEMBL1118335:
2024-05-04 23:31:23: CHEMBL3583198:
2024-05-04 23:31:24: CHEMBL3123721:
2024-05-04 23:31:25: CHEMBL3999675:
2024-05-04 23:31:26: CHEMBL1051811:
2024-05-04 23:31:27: CHEMBL3768642:
2024-05-04 23:31:27: CHEMBL655136:
2024-05-04 23:31:28: CHEMBL3072372:
2024-05-04 23:31:29: CHEMBL3625422:
2024-05-04 23:31:29: CHEMBL2400390:
2024-05-04 23:31:31: CHEMBL989570:
2024-05-04 23:31:32: CHEMBL2072116:
2024-05-04 23:31:32: CHEMBL968905:
2024-05-04 23:31:33: CHEMBL711804:
2024-05-04 23:31:34: CHEMBL1219015:
2024-05-04 23:31:35: CHEMBL1763053:
2024-05-04 23:31:35: CHEMBL3067105:
2024-05-04 23:31:36: CHEMBL3089774:
2024-05-04 23:31:37: CHEMBL1115306:
2024-05-04 23:31:37: CHEMBL909968:
2024-05-04 23:31:39: CHEMBL941164:
2024-05-04 23:31:40: CHEMBL3135839:
2024-05-04 23:31:40: CHEMBL961553:
2024-05-04 23:31:40: CHEMBL709777:
2024-05-04 23:31:41: CHEMBL656774:
2024-05-04 23:31:42: CHEMBL4043720:
2024-05-04 23:31:43: CHEMBL918692:
2024-05-04 23:31:43: CHEMBL972376:
2024-05-04 23:31:44: CHEMBL4220812:
2024-05-04 23:31:45: CHEMBL3811059:
2024-05-04 23:31:45: CHEMBL906554:
2024-05-04 23:31:46: CHEMBL1960697:
2024-05-04 23:31:47: CHEMBL1687598:
2024-05-04 23:31:47: CHEMBL1655237:
2024-05-04 23:31:48: CHEMBL2342763:
2024-05-04 23:31:49: CHEMBL2156781:
2024-05-04 23:31:49: CHEMBL1663702:
2024-05-04 23:31:50: CHEMBL3282591:
2024-05-04 23:31:50: CHEMBL2148600:
2024-05-04 23:31:51: CHEMBL1769611:
2024-05-04 23:31:52: CHEMBL2015317:
2024-05-04 23:31:53: CHEMBL2439550:
2024-05-04 23:31:54: CHEMBL4276507:
2024-05-04 23:31:55: CHEMBL899467:
2024-05-04 23:31:55: CHEMBL930744:
2024-05-04 23:31:56: CHEMBL995360:
2024-05-04 23:31:56: CHEMBL2039925:
2024-05-04 23:31:57: CHEMBL4197819:
2024-05-04 23:31:58: CHEMBL1047137:
2024-05-04 23:31:59: CHEMBL713546:
2024-05-04 23:31:59: CHEMBL837535:
2024-05-04 23:32:00: CHEMBL708489:
2024-05-04 23:32:01: CHEMBL955134:
2024-05-04 23:32:01: CHEMBL3292845:
2024-05-04 23:32:02: CHEMBL2439553:
2024-05-04 23:32:03: CHEMBL4477287:
2024-05-04 23:32:04: CHEMBL979180:
2024-05-04 23:32:05: CHEMBL908714:
2024-05-04 23:32:05: CHEMBL826375:
2024-05-04 23:32:06: CHEMBL2328533:
2024-05-04 23:32:07: CHEMBL3368957:
2024-05-04 23:32:07: CHEMBL899466:
Total norm: 38.008101591873256
Params with grad: 88762
Params without grad: 20
Current learning rate: [1.0, 0.010000000000000002]
Total norm: 41.52017422797514
Params with grad: 88762
Params without grad: 20
Current learning rate: [1.0, 0.011000000000000001]
Total norm: 44.534808134482454
Params with grad: 88762
Params without grad: 20
Current learning rate: [1.0, 0.012]
Total norm: 47.390864572289125
Params with grad: 88762
Params without grad: 20
Current learning rate: [1.0, 0.013000000000000001]
Total norm: 48.19317470874108
Params with grad: 88762
Params without grad: 20
Current learning rate: [1.0, 0.014000000000000002]
Total norm: 52.00235867610993
Params with grad: 88762
Params without grad: 20
Current learning rate: [1.0, 0.015]
Total norm: 54.57697203413073
Params with grad: 88762
Params without grad: 20
Current learning rate: [1.0, 0.016]
Total norm: 58.524197051941165
Params with grad: 88762
Params without grad: 20
Current learning rate: [1.0, 0.017]
Total norm: 61.78231396306103
Params with grad: 88762
Params without grad: 20
Current learning rate: [1.0, 0.018]
Total norm: 64.67281921864404
Params with grad: 88762
Params without grad: 20
Current learning rate: [1.0, 0.019000000000000003]
2024-05-04 23:33:43: CHEMBL804251:
2024-05-04 23:33:43: CHEMBL918692:
2024-05-04 23:33:43: CHEMBL4039493:
2024-05-04 23:33:43: CHEMBL1259552:
2024-05-04 23:33:43: CHEMBL4263092:
2024-05-04 23:33:43: CHEMBL3047091:
2024-05-04 23:33:43: CHEMBL4276506:
2024-05-04 23:33:43: CHEMBL713254:
2024-05-04 23:33:43: CHEMBL3627036:
2024-05-04 23:33:43: CHEMBL1100446:
2024-05-04 23:33:43: CHEMBL3116998:
2024-05-04 23:33:43: CHEMBL2211264:
2024-05-04 23:33:43: CHEMBL4263093:
2024-05-04 23:33:43: CHEMBL3744583:
2024-05-04 23:33:43: CHEMBL3578893:
2024-05-04 23:33:43: CHEMBL3122917:
2024-05-04 23:33:43: CHEMBL3063083:
2024-05-04 23:33:43: CHEMBL907802:
2024-05-04 23:33:43: CHEMBL1176948:
2024-05-04 23:33:43: CHEMBL936680:
2024-05-04 23:33:43: CHEMBL1177528:
2024-05-04 23:33:43: CHEMBL1918204:
2024-05-04 23:33:43: CHEMBL889770:
2024-05-04 23:33:43: CHEMBL3122349:
2024-05-04 23:33:43: CHEMBL901904:
2024-05-04 23:33:43: CHEMBL1763053:
2024-05-04 23:33:43: CHEMBL3387003:
2024-05-04 23:33:43: CHEMBL1033999:
2024-05-04 23:33:43: CHEMBL2033223:
2024-05-04 23:33:43: CHEMBL833079:
2024-05-04 23:33:43: CHEMBL3124021:
2024-05-04 23:33:43: CHEMBL1959200:
2024-05-04 23:33:43: CHEMBL1788062:
2024-05-04 23:33:43: CHEMBL960166:
2024-05-04 23:33:43: CHEMBL2319498:
2024-05-04 23:33:43: CHEMBL3240111:
2024-05-04 23:33:43: CHEMBL1006568:
2024-05-04 23:33:43: CHEMBL2432161:
2024-05-04 23:33:43: CHEMBL710864:
2024-05-04 23:33:43: CHEMBL710373:
2024-05-04 23:33:43: CHEMBL3994798:
2024-05-04 23:33:43: CHEMBL710534:
2024-05-04 23:33:43: CHEMBL914626:
2024-05-04 23:33:43: CHEMBL659279:
2024-05-04 23:33:43: CHEMBL887635:
2024-05-04 23:33:43: CHEMBL2342597:
2024-05-04 23:33:43: CHEMBL3999675:
2024-05-04 23:33:44: CHEMBL691343:
2024-05-04 23:33:44: CHEMBL1053168:
2024-05-04 23:33:44: CHEMBL1118335:
2024-05-04 23:33:44: CHEMBL1018957:
2024-05-04 23:33:44: CHEMBL899467:
2024-05-04 23:33:44: CHEMBL937510:
2024-05-04 23:33:44: CHEMBL1767670:
2024-05-04 23:33:44: CHEMBL979180:
2024-05-04 23:33:44: CHEMBL3778340:
2024-05-04 23:33:44: CHEMBL1024562:
2024-05-04 23:33:44: CHEMBL1109941:
2024-05-04 23:33:44: CHEMBL3072372:
2024-05-04 23:33:44: CHEMBL854478:
2024-05-04 23:33:44: CHEMBL3285745:
2024-05-04 23:33:44: CHEMBL903138:
2024-05-04 23:33:44: CHEMBL859285:
2024-05-04 23:33:44: CHEMBL2044336:
2024-05-04 23:33:44: CHEMBL653803:
2024-05-04 23:33:44: CHEMBL995902:
2024-05-04 23:33:44: CHEMBL2173189:
2024-05-04 23:33:44: CHEMBL4194448:
2024-05-04 23:33:44: CHEMBL4002825:
2024-05-04 23:33:44: CHEMBL3293254:
2024-05-04 23:33:44: CHEMBL3129213:
2024-05-04 23:33:44: CHEMBL3282591:
2024-05-04 23:33:45: CHEMBL1107214:
2024-05-04 23:33:45: CHEMBL903137:
2024-05-04 23:33:45: CHEMBL1011648:
2024-05-04 23:33:45: CHEMBL907807:
2024-05-04 23:33:45: CHEMBL1932094:
2024-05-04 23:33:45: CHEMBL1809301:
2024-05-04 23:33:45: CHEMBL1815047:
2024-05-04 23:33:45: CHEMBL1033996:
2024-05-04 23:33:45: CHEMBL814078:
2024-05-04 23:33:45: CHEMBL1921081:
2024-05-04 23:33:45: CHEMBL887332:
2024-05-04 23:33:45: CHEMBL1051810:
2024-05-04 23:33:45: CHEMBL2162371:
2024-05-04 23:33:45: CHEMBL758925:
2024-05-04 23:33:45: CHEMBL3089774:
2024-05-04 23:33:45: CHEMBL1051809:
2024-05-04 23:33:45: CHEMBL889768:
2024-05-04 23:33:45: CHEMBL3794968:
2024-05-04 23:33:45: CHEMBL2025518:
2024-05-04 23:33:45: CHEMBL3117566:
2024-05-04 23:33:45: CHEMBL1803361:
2024-05-04 23:33:45: CHEMBL897415:
2024-05-04 23:33:45: CHEMBL4195632:
2024-05-04 23:33:46: CHEMBL4134181:
2024-05-04 23:33:46: CHEMBL2072118:
2024-05-04 23:33:46: CHEMBL833378:
2024-05-04 23:33:46: CHEMBL689145:
2024-05-04 23:33:46: CHEMBL1034017:
2024-05-04 23:33:46: CHEMBL1018953:
2024-05-04 23:33:46: CHEMBL712878:
2024-05-04 23:33:46: CHEMBL1646836:
2024-05-04 23:33:46: CHEMBL3637832:
2024-05-04 23:33:46: CHEMBL2342763:
2024-05-04 23:33:46: CHEMBL1913092:
2024-05-04 23:33:46: CHEMBL2339580:
2024-05-04 23:33:46: CHEMBL900552:
2024-05-04 23:33:46: CHEMBL995360:
2024-05-04 23:33:46: CHEMBL1219014:
2024-05-04 23:33:46: CHEMBL706947:
2024-05-04 23:33:46: CHEMBL3295663:
2024-05-04 23:33:46: CHEMBL3128794:
2024-05-04 23:33:46: CHEMBL968900:
2024-05-04 23:33:46: CHEMBL2067378:
2024-05-04 23:33:46: CHEMBL918457:
2024-05-04 23:33:46: CHEMBL1033991:
2024-05-04 23:33:46: CHEMBL690079:
2024-05-04 23:33:46: CHEMBL1108106:
2024-05-04 23:33:47: CHEMBL2351677:
2024-05-04 23:33:47: CHEMBL3782449:
2024-05-04 23:33:47: CHEMBL862229:
2024-05-04 23:33:47: CHEMBL1115072:
2024-05-04 23:33:47: CHEMBL4269007:
2024-05-04 23:33:47: CHEMBL3811059:
2024-05-04 23:33:47: CHEMBL950929:
2024-05-04 23:33:47: CHEMBL759088:
2024-05-04 23:33:47: CHEMBL3375068:
2024-05-04 23:33:47: CHEMBL696950:
2024-05-04 23:33:47: CHEMBL4276507:
2024-05-04 23:33:47: CHEMBL984427:
2024-05-04 23:33:47: CHEMBL3122511:
2024-05-04 23:33:47: CHEMBL2353302:
2024-05-04 23:33:49: CHEMBL865135:
2024-05-04 23:33:50: CHEMBL860970:
2024-05-04 23:33:52: CHEMBL676912:
2024-05-04 23:33:52: CHEMBL2379640:
2024-05-04 23:33:54: CHEMBL1228542:
2024-05-04 23:33:54: CHEMBL1821364:
2024-05-04 23:33:55: CHEMBL1646260:
2024-05-04 23:33:56: CHEMBL1053167:
2024-05-04 23:33:57: CHEMBL4022472:
2024-05-04 23:33:57: CHEMBL4039492:
2024-05-04 23:33:58: CHEMBL3295320:
2024-05-04 23:33:59: CHEMBL4276508:
2024-05-04 23:33:59: CHEMBL1614018:
2024-05-04 23:34:00: CHEMBL1051811:
2024-05-04 23:34:01: CHEMBL866685:
2024-05-04 23:34:02: CHEMBL2059948:
2024-05-04 23:34:03: CHEMBL3389957:
2024-05-04 23:34:04: CHEMBL944781:
2024-05-04 23:34:05: CHEMBL4197926:
2024-05-04 23:34:06: CHEMBL827425:
2024-05-04 23:34:08: CHEMBL945414:
2024-05-04 23:34:09: CHEMBL710381:
2024-05-04 23:34:09: CHEMBL1022218:
2024-05-04 23:34:10: CHEMBL940416:
2024-05-04 23:34:10: CHEMBL697276:
2024-05-04 23:34:11: CHEMBL3595591:
2024-05-04 23:34:12: CHEMBL824904:
Total norm: 68.48600008885862
Params with grad: 88762
Params without grad: 20
Current learning rate: [1.0, 0.020000000000000004]
2024-05-04 23:34:13:  Step 0020 || Mean metrics so far: loss: 1.39754, avg_prec: 0.75630, kappa: 0.22755, acc: 0.66574, auc: 0.66524 || This window: loss: 1.39754, avg_prec: 0.75630, kappa: 0.22755, acc: 0.66574, auc: 0.66524
2024-05-04 23:34:13: CHEMBL1925635:
2024-05-04 23:34:14: CHEMBL856486:
2024-05-04 23:34:15: CHEMBL1116449:
2024-05-04 23:34:15: CHEMBL949416:
2024-05-04 23:34:16: CHEMBL1959201:
2024-05-04 23:34:17: CHEMBL710372:
2024-05-04 23:34:17: CHEMBL1815995:
2024-05-04 23:34:18: CHEMBL2406254:
2024-05-04 23:34:18: CHEMBL1033994:
2024-05-04 23:34:19: CHEMBL853640:
2024-05-04 23:34:20: CHEMBL4028610:
2024-05-04 23:34:21: CHEMBL833275:
2024-05-04 23:34:21: CHEMBL860972:
2024-05-04 23:34:22: CHEMBL866752:
2024-05-04 23:34:23: CHEMBL4264727:
2024-05-04 23:34:24: CHEMBL3227331:
2024-05-04 23:34:24: CHEMBL655136:
2024-05-04 23:34:25: CHEMBL711490:
2024-05-04 23:34:26: CHEMBL1794490:
2024-05-04 23:34:26: CHEMBL3768642:
2024-05-04 23:34:27: CHEMBL864208:
2024-05-04 23:34:28: CHEMBL2167912:
2024-05-04 23:34:29: CHEMBL1769825:
2024-05-04 23:34:30: CHEMBL1033787:
2024-05-04 23:34:30: CHEMBL885174:
2024-05-04 23:34:31: CHEMBL972376:
2024-05-04 23:34:32: CHEMBL906490:
2024-05-04 23:34:33: CHEMBL1046975:
2024-05-04 23:34:33: CHEMBL3291715:
2024-05-04 23:34:34: CHEMBL975243:
2024-05-04 23:34:35: CHEMBL934248:
2024-05-04 23:34:35: CHEMBL1960697:
2024-05-04 23:34:36: CHEMBL1655239:
2024-05-04 23:34:37: CHEMBL1217314:
2024-05-04 23:34:38: CHEMBL1059735:
2024-05-04 23:34:39: CHEMBL1063960:
2024-05-04 23:34:39: CHEMBL3383460:
2024-05-04 23:34:40: CHEMBL832523:
2024-05-04 23:34:41: CHEMBL949071:
2024-05-04 23:34:42: CHEMBL3812459:
2024-05-04 23:34:42: CHEMBL4002824:
2024-05-04 23:34:42: CHEMBL4011461:
2024-05-04 23:34:43: CHEMBL3368957:
2024-05-04 23:34:44: CHEMBL1039164:
2024-05-04 23:34:44: CHEMBL3384577:
2024-05-04 23:34:45: CHEMBL3055209:
2024-05-04 23:34:46: CHEMBL2092203:
2024-05-04 23:34:46: CHEMBL906554:
2024-05-04 23:34:47: CHEMBL887561:
2024-05-04 23:34:48: CHEMBL834743:
2024-05-04 23:34:48: CHEMBL1035508:
2024-05-04 23:34:49: CHEMBL837535:
2024-05-04 23:34:49: CHEMBL3611931:
2024-05-04 23:34:50: CHEMBL1654779:
2024-05-04 23:34:51: CHEMBL1219015:
2024-05-04 23:34:52: CHEMBL3592342:
2024-05-04 23:34:52: CHEMBL902989:
2024-05-04 23:34:53: CHEMBL3106982:
2024-05-04 23:34:54: CHEMBL906386:
2024-05-04 23:34:54: CHEMBL4002828:
2024-05-04 23:34:55: CHEMBL4253251:
2024-05-04 23:34:57: CHEMBL903139:
2024-05-04 23:34:58: CHEMBL2320857:
2024-05-04 23:34:58: CHEMBL2395313:
2024-05-04 23:34:59: CHEMBL984613:
2024-05-04 23:34:59: CHEMBL1936129:
2024-05-04 23:35:00: CHEMBL4270097:
2024-05-04 23:35:02: CHEMBL972377:
2024-05-04 23:35:02: CHEMBL3412552:
2024-05-04 23:35:03: CHEMBL956772:
2024-05-04 23:35:04: CHEMBL1047137:
2024-05-04 23:35:04: CHEMBL837467:
2024-05-04 23:35:05: CHEMBL684710:
2024-05-04 23:35:06: CHEMBL4014622:
2024-05-04 23:35:06: CHEMBL4008248:
2024-05-04 23:35:07: CHEMBL703466:
2024-05-04 23:35:07: CHEMBL3266569:
2024-05-04 23:35:08: CHEMBL871030:
2024-05-04 23:35:09: CHEMBL894309:
2024-05-04 23:35:10: CHEMBL715738:
2024-05-04 23:35:11: CHEMBL837207:
2024-05-04 23:35:12: CHEMBL2156781:
2024-05-04 23:35:14: CHEMBL1646095:
2024-05-04 23:35:15: CHEMBL3389542:
2024-05-04 23:35:16: CHEMBL1047854:
2024-05-04 23:35:17: CHEMBL1737867:
2024-05-04 23:35:17: CHEMBL711617:
2024-05-04 23:35:17: CHEMBL3292952:
2024-05-04 23:35:18: CHEMBL3380429:
2024-05-04 23:35:19: CHEMBL1941610:
2024-05-04 23:35:19: CHEMBL892896:
2024-05-04 23:35:20: CHEMBL3761618:
2024-05-04 23:35:21: CHEMBL3738561:
2024-05-04 23:35:22: CHEMBL2051603:
2024-05-04 23:35:22: CHEMBL4179593:
2024-05-04 23:35:23: CHEMBL2400409:
2024-05-04 23:35:24: CHEMBL1776383:
2024-05-04 23:35:24: CHEMBL708445:
2024-05-04 23:35:25: CHEMBL924692:
2024-05-04 23:35:26: CHEMBL3778315:
2024-05-04 23:35:26: CHEMBL2015865:
2024-05-04 23:35:27: CHEMBL941164:
2024-05-04 23:35:28: CHEMBL683837:
2024-05-04 23:35:29: CHEMBL1219004:
2024-05-04 23:35:29: CHEMBL897796:
2024-05-04 23:35:30: CHEMBL1017067:
2024-05-04 23:35:31: CHEMBL705091:
2024-05-04 23:35:31: CHEMBL713409:
2024-05-04 23:35:32: CHEMBL3583198:
2024-05-04 23:35:33: CHEMBL3625422:
2024-05-04 23:35:34: CHEMBL712346:
2024-05-04 23:35:34: CHEMBL4721621:
2024-05-04 23:35:35: CHEMBL656774:
2024-05-04 23:35:36: CHEMBL4276505:
2024-05-04 23:35:37: CHEMBL1960696:
2024-05-04 23:35:37: CHEMBL3636663:
2024-05-04 23:35:38: CHEMBL1033990:
2024-05-04 23:35:39: CHEMBL3123238:
2024-05-04 23:35:40: CHEMBL4008247:
2024-05-04 23:35:40: CHEMBL3767404:
2024-05-04 23:35:41: CHEMBL3368544:
2024-05-04 23:35:42: CHEMBL902229:
2024-05-04 23:35:42: CHEMBL3060860:
2024-05-04 23:35:43: CHEMBL1635612:
2024-05-04 23:35:43: CHEMBL1654783:
2024-05-04 23:35:44: CHEMBL4479167:
2024-05-04 23:35:45: CHEMBL3807629:
2024-05-04 23:35:45: CHEMBL4477287:
2024-05-04 23:35:46: CHEMBL854523:
2024-05-04 23:35:47: CHEMBL2390645:
2024-05-04 23:35:48: CHEMBL961553:
2024-05-04 23:35:48: CHEMBL4187321:
2024-05-04 23:35:48: CHEMBL1913093:
2024-05-04 23:35:49: CHEMBL1000357:
2024-05-04 23:35:50: CHEMBL946064:
2024-05-04 23:35:51: CHEMBL1019290:
2024-05-04 23:35:54: CHEMBL657413:
2024-05-04 23:35:55: CHEMBL653610:
2024-05-04 23:35:56: CHEMBL833301:
2024-05-04 23:35:56: CHEMBL922474:
2024-05-04 23:35:57: CHEMBL728516:
2024-05-04 23:35:58: CHEMBL896487:
2024-05-04 23:35:58: CHEMBL955136:
2024-05-04 23:35:59: CHEMBL3078730:
2024-05-04 23:36:00: CHEMBL3295870:
2024-05-04 23:36:01: CHEMBL2439550:
2024-05-04 23:36:01: CHEMBL921335:
2024-05-04 23:36:02: CHEMBL3122259:
2024-05-04 23:36:03: CHEMBL3292846:
2024-05-04 23:36:03: CHEMBL899618:
2024-05-04 23:36:03: CHEMBL1034018:
2024-05-04 23:36:04: CHEMBL3292845:
2024-05-04 23:36:05: CHEMBL946070:
2024-05-04 23:36:05: CHEMBL1073156:
2024-05-04 23:36:06: CHEMBL3418701:
2024-05-04 23:36:06: CHEMBL3999676:
2024-05-04 23:36:08: CHEMBL689615:
2024-05-04 23:36:08: CHEMBL4719106:
2024-05-04 23:36:09: CHEMBL705093:
2024-05-04 23:36:09: CHEMBL864455:
2024-05-04 23:36:10: CHEMBL1020690:
2024-05-04 23:36:10: CHEMBL4028622:
2024-05-04 23:36:11: CHEMBL2213258:
2024-05-04 23:36:12: CHEMBL656768:
2024-05-04 23:36:12: CHEMBL833076:
2024-05-04 23:36:13: CHEMBL1794398:
2024-05-04 23:36:14: CHEMBL2379584:
2024-05-04 23:36:15: CHEMBL713420:
2024-05-04 23:36:15: CHEMBL1817391:
2024-05-04 23:36:16: CHEMBL1024879:
2024-05-04 23:36:17: CHEMBL1024326:
2024-05-04 23:36:17: CHEMBL930744:
2024-05-04 23:36:18: CHEMBL968903:
2024-05-04 23:36:19: CHEMBL3124294:
2024-05-04 23:36:20: CHEMBL1961322:
2024-05-04 23:36:20: CHEMBL2043755:
2024-05-04 23:36:21: CHEMBL1664688:
2024-05-04 23:36:21: CHEMBL2439553:
2024-05-04 23:36:21: CHEMBL2344963:
2024-05-04 23:36:22: CHEMBL684103:
2024-05-04 23:36:23: CHEMBL1815005:
2024-05-04 23:36:23: CHEMBL955850:
2024-05-04 23:36:24: CHEMBL957277:
2024-05-04 23:36:25: CHEMBL1056891:
2024-05-04 23:36:25: CHEMBL2439404:
2024-05-04 23:36:26: CHEMBL2404680:
2024-05-04 23:36:27: CHEMBL3418917:
2024-05-04 23:36:28: CHEMBL2405555:
2024-05-04 23:36:29: CHEMBL2400390:
2024-05-04 23:36:30: CHEMBL907801:
2024-05-04 23:36:31: CHEMBL655544:
2024-05-04 23:36:31: CHEMBL833117:
2024-05-04 23:36:32: CHEMBL2154557:
2024-05-04 23:36:33: CHEMBL833118:
2024-05-04 23:36:34: CHEMBL2051598:
2024-05-04 23:36:35: CHEMBL4194391:
2024-05-04 23:36:36: CHEMBL956777:
2024-05-04 23:36:36: CHEMBL3123239:
2024-05-04 23:36:37: CHEMBL3396060:
2024-05-04 23:36:38: CHEMBL4136409:
2024-05-04 23:36:39: CHEMBL1058083:
2024-05-04 23:36:39: CHEMBL3131181:
2024-05-04 23:36:40: CHEMBL992597:
2024-05-04 23:36:41: CHEMBL710084:
2024-05-04 23:36:41: CHEMBL3073615:
2024-05-04 23:36:42: CHEMBL1260089:
2024-05-04 23:36:42: CHEMBL2037884:
2024-05-04 23:36:43: CHEMBL3807925:
2024-05-04 23:36:43: CHEMBL922937:
2024-05-04 23:36:44: CHEMBL3129137:
2024-05-04 23:36:45: CHEMBL934825:
2024-05-04 23:36:46: CHEMBL904573:
2024-05-04 23:36:47: CHEMBL873604:
2024-05-04 23:36:48: CHEMBL685389:
2024-05-04 23:36:49: CHEMBL968905:
2024-05-04 23:36:50: CHEMBL1663615:
2024-05-04 23:36:50: CHEMBL907804:
2024-05-04 23:36:51: CHEMBL804248:
2024-05-04 23:36:52: CHEMBL980716:
2024-05-04 23:36:53: CHEMBL3227446:
2024-05-04 23:36:53: CHEMBL933048:
2024-05-04 23:36:54: CHEMBL1273622:
2024-05-04 23:36:54: CHEMBL899466:
2024-05-04 23:36:55: CHEMBL3599625:
2024-05-04 23:36:55: CHEMBL1007795:
2024-05-04 23:36:57: CHEMBL1037279:
2024-05-04 23:36:57: CHEMBL4273395:
2024-05-04 23:36:57: CHEMBL834635:
2024-05-04 23:36:58: CHEMBL2019245:
2024-05-04 23:36:59: CHEMBL1067568:
2024-05-04 23:37:03: CHEMBL2148600:
2024-05-04 23:37:03: CHEMBL956847:
2024-05-04 23:37:04: CHEMBL2390255:
2024-05-04 23:37:04: CHEMBL3782490:
2024-05-04 23:37:05: CHEMBL1034012:
2024-05-04 23:37:06: CHEMBL1054762:
2024-05-04 23:37:06: CHEMBL2447603:
2024-05-04 23:37:07: CHEMBL894948:
2024-05-04 23:37:08: CHEMBL2379829:
2024-05-04 23:37:08: CHEMBL1041705:
2024-05-04 23:37:08: CHEMBL3100980:
2024-05-04 23:37:09: CHEMBL2328533:
2024-05-04 23:37:10: CHEMBL1803360:
2024-05-04 23:37:10: CHEMBL3069079:
2024-05-04 23:37:11: CHEMBL1654778:
